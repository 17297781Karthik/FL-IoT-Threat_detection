{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":897617,"sourceType":"datasetVersion","datasetId":480187}],"dockerImageVersionId":30357,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n!pip install elephas\n","metadata":{"papermill":{"duration":133.98401,"end_time":"2023-04-01T02:50:41.097451","exception":false,"start_time":"2023-04-01T02:48:27.113441","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:40:52.155004Z","iopub.execute_input":"2023-04-03T12:40:52.155519Z","iopub.status.idle":"2023-04-03T12:43:01.865128Z","shell.execute_reply.started":"2023-04-03T12:40:52.155417Z","shell.execute_reply":"2023-04-03T12:43:01.863362Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.103685,"end_time":"2023-04-01T02:50:41.239211","exception":false,"start_time":"2023-04-01T02:50:41.135526","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:01.867669Z","iopub.execute_input":"2023-04-03T12:43:01.868056Z","iopub.status.idle":"2023-04-03T12:43:01.926685Z","shell.execute_reply.started":"2023-04-03T12:43:01.868019Z","shell.execute_reply":"2023-04-03T12:43:01.925259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"papermill":{"duration":0.047734,"end_time":"2023-04-01T02:50:41.324752","exception":false,"start_time":"2023-04-01T02:50:41.277018","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:01.928485Z","iopub.execute_input":"2023-04-03T12:43:01.928854Z","iopub.status.idle":"2023-04-03T12:43:01.93471Z","shell.execute_reply.started":"2023-04-03T12:43:01.928822Z","shell.execute_reply":"2023-04-03T12:43:01.933351Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import SparkSession \nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext\n\nspark = SparkSession.builder.appName('Spark Federated DL').getOrCreate()\n\nmaxlength = 1000\nspark.conf.set(\"spark.sql.debug.maxToStringFields\", maxlength)","metadata":{"papermill":{"duration":7.539565,"end_time":"2023-04-01T02:50:48.903195","exception":false,"start_time":"2023-04-01T02:50:41.36363","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:01.937775Z","iopub.execute_input":"2023-04-03T12:43:01.938174Z","iopub.status.idle":"2023-04-03T12:43:09.069131Z","shell.execute_reply.started":"2023-04-03T12:43:01.938139Z","shell.execute_reply":"2023-04-03T12:43:09.068087Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_df = pd.read_csv(\"/kaggle/input/nbaiot-dataset/features.csv\")\nfeatures_df","metadata":{"papermill":{"duration":0.098191,"end_time":"2023-04-01T02:50:49.041686","exception":false,"start_time":"2023-04-01T02:50:48.943495","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:09.070409Z","iopub.execute_input":"2023-04-03T12:43:09.070757Z","iopub.status.idle":"2023-04-03T12:43:09.115521Z","shell.execute_reply.started":"2023-04-03T12:43:09.070722Z","shell.execute_reply":"2023-04-03T12:43:09.114212Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary_df = pd.read_csv(\"/kaggle/input/nbaiot-dataset/data_summary.csv\")\nsummary_df","metadata":{"papermill":{"duration":0.074006,"end_time":"2023-04-01T02:50:49.156552","exception":false,"start_time":"2023-04-01T02:50:49.082546","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:09.117503Z","iopub.execute_input":"2023-04-03T12:43:09.118284Z","iopub.status.idle":"2023-04-03T12:43:09.142309Z","shell.execute_reply.started":"2023-04-03T12:43:09.118234Z","shell.execute_reply":"2023-04-03T12:43:09.141052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary_df = pd.read_csv(\"/kaggle/input/nbaiot-dataset/device_info.csv\")\nsummary_df","metadata":{"papermill":{"duration":0.070389,"end_time":"2023-04-01T02:50:49.268708","exception":false,"start_time":"2023-04-01T02:50:49.198319","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:09.143783Z","iopub.execute_input":"2023-04-03T12:43:09.14416Z","iopub.status.idle":"2023-04-03T12:43:09.161199Z","shell.execute_reply.started":"2023-04-03T12:43:09.1441Z","shell.execute_reply":"2023-04-03T12:43:09.160024Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_files = [\"/kaggle/input/nbaiot-dataset/features.csv\",\n             \"/kaggle/input/nbaiot-dataset/data_summary.csv\",\n             \"/kaggle/input/nbaiot-dataset/device_info.csv\"]","metadata":{"papermill":{"duration":0.049089,"end_time":"2023-04-01T02:50:49.372503","exception":false,"start_time":"2023-04-01T02:50:49.323414","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:09.162996Z","iopub.execute_input":"2023-04-03T12:43:09.163395Z","iopub.status.idle":"2023-04-03T12:43:09.168404Z","shell.execute_reply.started":"2023-04-03T12:43:09.163362Z","shell.execute_reply":"2023-04-03T12:43:09.167151Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\n\ncsv_files = glob.glob(\"/kaggle/input/nbaiot-dataset/*.csv\")\n[csv_files.remove(f) for f in out_files];","metadata":{"papermill":{"duration":0.053486,"end_time":"2023-04-01T02:50:49.464596","exception":false,"start_time":"2023-04-01T02:50:49.41111","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:09.170238Z","iopub.execute_input":"2023-04-03T12:43:09.170744Z","iopub.status.idle":"2023-04-03T12:43:09.181827Z","shell.execute_reply.started":"2023-04-03T12:43:09.170705Z","shell.execute_reply":"2023-04-03T12:43:09.180387Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, FloatType\n\nschema = StructType([ \n    StructField(\"MI_dir_L5_weight\", DoubleType(), True), \n    StructField(\"MI_dir_L5_mean\", DoubleType(), True), \n    StructField(\"MI_dir_L5_variance\", DoubleType(), True),\n    StructField(\"MI_dir_L3_weight\", DoubleType(), True),\n    StructField(\"MI_dir_L3_mean\", DoubleType(), True),\n    StructField(\"MI_dir_L3_variance\", DoubleType(), True),\n    StructField(\"MI_dir_L1_weight\", DoubleType(), True),\n    StructField(\"MI_dir_L1_mean\", DoubleType(), True),\n    StructField(\"MI_dir_L1_variance\", DoubleType(), True), \n    StructField(\"MI_dir_L0_1_weight\", DoubleType(), True),\n    StructField(\"MI_dir_L0_1_mean\", DoubleType(), True),\n    StructField(\"MI_dir_L0_1_variance\", DoubleType(), True),\n    StructField(\"MI_dir_L0_01_weight\", DoubleType(), True),\n    StructField(\"MI_dir_L0_01_mean\", DoubleType(), True),\n    StructField(\"MI_dir_L0_01_variance\", DoubleType(), True),\n    StructField(\"H_L5_weight\", DoubleType(), True),\n    StructField(\"H_L5_mean\", DoubleType(), True),\n    StructField(\"H_L5_variance\", DoubleType(), True),\n    StructField(\"H_L3_weight\", DoubleType(), True),\n    StructField(\"H_L3_mean\", DoubleType(), True),\n    StructField(\"H_L3_variance\", DoubleType(), True),   \n    StructField(\"H_L1_weight\", DoubleType(), True),\n    StructField(\"H_L1_mean\", DoubleType(), True),\n    StructField(\"H_L1_variance\", DoubleType(), True),   \n    StructField(\"H_L0_1_weight\", DoubleType(), True),\n    StructField(\"H_L0_1_mean\", DoubleType(), True),\n    StructField(\"H_L0_1_variance\", DoubleType(), True),\n    StructField(\"H_L0_01_weight\", DoubleType(), True),\n    StructField(\"H_L0_01_mean\", DoubleType(), True),\n    StructField(\"H_L0_01_variance\", DoubleType(), True), \n    StructField(\"HH_L5_weight\", DoubleType(), True),\n    StructField(\"HH_L5_mean\", DoubleType(), True),\n    StructField(\"HH_L5_std\", DoubleType(), True),\n    StructField(\"HH_L5_magnitude\", DoubleType(), True),\n    StructField(\"HH_L5_radius\", DoubleType(), True),\n    StructField(\"HH_L5_covariance\", DoubleType(), True),\n    StructField(\"HH_L5_pcc\", DoubleType(), True),   \n    StructField(\"HH_L3_weight\", DoubleType(), True),\n    StructField(\"HH_L3_mean\", DoubleType(), True),\n    StructField(\"HH_L3_std\", DoubleType(), True),\n    StructField(\"HH_L3_magnitude\", DoubleType(), True),\n    StructField(\"HH_L3_radius\", DoubleType(), True),\n    StructField(\"HH_L3_covariance\", DoubleType(), True),\n    StructField(\"HH_L3_pcc\", DoubleType(), True),  \n    StructField(\"HH_L1_weight\", DoubleType(), True),\n    StructField(\"HH_L1_mean\", DoubleType(), True),\n    StructField(\"HH_L1_std\", DoubleType(), True),\n    StructField(\"HH_L1_magnitude\", DoubleType(), True),\n    StructField(\"HH_L1_radius\", DoubleType(), True),\n    StructField(\"HH_L1_covariance\", DoubleType(), True),\n    StructField(\"HH_L1_pcc\", DoubleType(), True),\n    StructField(\"HH_L0_1_weight\", DoubleType(), True),\n    StructField(\"HH_L0_1_mean\", DoubleType(), True),\n    StructField(\"HH_L0_1_std\", DoubleType(), True),\n    StructField(\"HH_L0_1_magnitude\", DoubleType(), True),\n    StructField(\"HH_L0_1_radius\", DoubleType(), True),\n    StructField(\"HH_L0_1_covariance\", DoubleType(), True),\n    StructField(\"HH_L0_1_pcc\", DoubleType(), True),    \n    StructField(\"HH_L0_01_weight\", DoubleType(), True),\n    StructField(\"HH_L0_01_mean\", DoubleType(), True),\n    StructField(\"HH_L0_01_std\", DoubleType(), True),\n    StructField(\"HH_L0_01_magnitude\", DoubleType(), True),\n    StructField(\"HH_L0_01_radius\", DoubleType(), True),\n    StructField(\"HH_L0_01_covariance\", DoubleType(), True),\n    StructField(\"HH_L0_01_pcc\", DoubleType(), True),\n    StructField(\"HH_jit_L5_weight\", DoubleType(), True),\n    StructField(\"HH_jit_L5_mean\", DoubleType(), True),\n    StructField(\"HH_jit_L5_variance\", DoubleType(), True),\n    StructField(\"HH_jit_L3_weight\", DoubleType(), True),\n    StructField(\"HH_jit_L3_mean\", DoubleType(), True),\n    StructField(\"HH_jit_L3_variance\", DoubleType(), True),\n    StructField(\"HH_jit_L1_weight\", DoubleType(), True),\n    StructField(\"HH_jit_L1_mean\", DoubleType(), True),\n    StructField(\"HH_jit_L1_variance\", DoubleType(), True),\n    StructField(\"HH_jit_L0_1_weight\", DoubleType(), True),\n    StructField(\"HH_jit_L0_1_mean\", DoubleType(), True),\n    StructField(\"HH_jit_L0_1_variance\", DoubleType(), True),\n    StructField(\"HH_jit_L0_01_weight\", DoubleType(), True),\n    StructField(\"HH_jit_L0_01_mean\", DoubleType(), True),\n    StructField(\"HH_jit_L0_01_variance\", DoubleType(), True),\n    StructField(\"HpHp_L5_weight\", DoubleType(), True),\n    StructField(\"HpHp_L5_mean\", DoubleType(), True),\n    StructField(\"HpHp_L5_std\", DoubleType(), True),\n    StructField(\"HpHp_L5_magnitude\", DoubleType(), True),\n    StructField(\"HpHp_L5_radius\", DoubleType(), True),\n    StructField(\"HpHp_L5_covariance\", DoubleType(), True),\n    StructField(\"HpHp_L5_pcc\", DoubleType(), True),\n    StructField(\"HpHp_L3_weight\", DoubleType(), True),\n    StructField(\"HpHp_L3_mean\", DoubleType(), True),\n    StructField(\"HpHp_L3_std\", DoubleType(), True),\n    StructField(\"HpHp_L3_magnitude\", DoubleType(), True),\n    StructField(\"HpHp_L3_radius\", DoubleType(), True),\n    StructField(\"HpHp_L3_covariance\", DoubleType(), True),\n    StructField(\"HpHp_L3_pcc\", DoubleType(), True),\n    StructField(\"HpHp_L1_weight\", DoubleType(), True),\n    StructField(\"HpHp_L1_mean\", DoubleType(), True),\n    StructField(\"HpHp_L1_std\", DoubleType(), True),\n    StructField(\"HpHp_L1_magnitude\", DoubleType(), True),\n    StructField(\"HpHp_L1_radius\", DoubleType(), True),\n    StructField(\"HpHp_L1_covariance\", DoubleType(), True),\n    StructField(\"HpHp_L1_pcc\", DoubleType(), True),\n    StructField(\"HpHp_L0_1_weight\", DoubleType(), True),\n    StructField(\"HpHp_L0_1_mean\", DoubleType(), True),\n    StructField(\"HpHp_L0_1_std\", DoubleType(), True),\n    StructField(\"HpHp_L0_1_magnitude\", DoubleType(), True),\n    StructField(\"HpHp_L0_1_radius\", DoubleType(), True),\n    StructField(\"HpHp_L0_1_covariance\", DoubleType(), True),\n    StructField(\"HpHp_L0_1_pcc\", DoubleType(), True),\n    StructField(\"HpHp_L0_01_weight\", DoubleType(), True),\n    StructField(\"HpHp_L0_01_mean\", DoubleType(), True),\n    StructField(\"HpHp_L0_01_std\", DoubleType(), True),\n    StructField(\"HpHp_L0_01_magnitude\", DoubleType(), True),\n    StructField(\"HpHp_L0_01_radius\", DoubleType(), True),\n    StructField(\"HpHp_L0_01_covariance\", DoubleType(), True),\n    StructField(\"HpHp_L0_01_pcc\", DoubleType(), True)\n  ])\n \nlen(schema)","metadata":{"papermill":{"duration":0.088696,"end_time":"2023-04-01T02:50:49.593405","exception":false,"start_time":"2023-04-01T02:50:49.504709","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:09.187197Z","iopub.execute_input":"2023-04-03T12:43:09.187896Z","iopub.status.idle":"2023-04-03T12:43:09.228874Z","shell.execute_reply.started":"2023-04-03T12:43:09.187858Z","shell.execute_reply":"2023-04-03T12:43:09.227944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=spark.read.csv(csv_files,\n                  header=False, \n                  schema=schema\n                 )\n\ndf.printSchema()\n","metadata":{"papermill":{"duration":5.849711,"end_time":"2023-04-01T02:50:55.484298","exception":false,"start_time":"2023-04-01T02:50:49.634587","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:09.230212Z","iopub.execute_input":"2023-04-03T12:43:09.230787Z","iopub.status.idle":"2023-04-03T12:43:14.839768Z","shell.execute_reply.started":"2023-04-03T12:43:09.230751Z","shell.execute_reply":"2023-04-03T12:43:14.838522Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.describe().toPandas().transpose()","metadata":{"papermill":{"duration":0.053875,"end_time":"2023-04-01T02:50:55.5802","exception":false,"start_time":"2023-04-01T02:50:55.526325","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:14.841053Z","iopub.execute_input":"2023-04-03T12:43:14.841514Z","iopub.status.idle":"2023-04-03T12:43:14.850009Z","shell.execute_reply.started":"2023-04-03T12:43:14.841469Z","shell.execute_reply":"2023-04-03T12:43:14.848729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.na.drop()","metadata":{"papermill":{"duration":0.269131,"end_time":"2023-04-01T02:50:55.889265","exception":false,"start_time":"2023-04-01T02:50:55.620134","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:14.851892Z","iopub.execute_input":"2023-04-03T12:43:14.852689Z","iopub.status.idle":"2023-04-03T12:43:15.077456Z","shell.execute_reply.started":"2023-04-03T12:43:14.852636Z","shell.execute_reply":"2023-04-03T12:43:15.076205Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from  pyspark.sql.functions import input_file_name\n\ndf = df.withColumn(\"path\", input_file_name())","metadata":{"papermill":{"duration":0.20952,"end_time":"2023-04-01T02:50:56.158395","exception":false,"start_time":"2023-04-01T02:50:55.948875","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.078751Z","iopub.execute_input":"2023-04-03T12:43:15.07921Z","iopub.status.idle":"2023-04-03T12:43:15.214864Z","shell.execute_reply.started":"2023-04-03T12:43:15.079164Z","shell.execute_reply":"2023-04-03T12:43:15.213732Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import col, udf\n\ndef getType(path):\n    sampleType = path.split('/')[-1].split('.')[1:-1]\n    return \"_\".join(sampleType)\n\ngetTypeUDF = udf(lambda x:getType(x),StringType()) \ndf = df.withColumn(\"type\", getTypeUDF(col(\"path\")))","metadata":{"papermill":{"duration":0.166789,"end_time":"2023-04-01T02:50:56.366627","exception":false,"start_time":"2023-04-01T02:50:56.199838","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.216277Z","iopub.execute_input":"2023-04-03T12:43:15.217695Z","iopub.status.idle":"2023-04-03T12:43:15.342396Z","shell.execute_reply.started":"2023-04-03T12:43:15.217641Z","shell.execute_reply":"2023-04-03T12:43:15.341058Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop(\"path\")","metadata":{"papermill":{"duration":0.109129,"end_time":"2023-04-01T02:50:56.535707","exception":false,"start_time":"2023-04-01T02:50:56.426578","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.343627Z","iopub.execute_input":"2023-04-03T12:43:15.344123Z","iopub.status.idle":"2023-04-03T12:43:15.406743Z","shell.execute_reply.started":"2023-04-03T12:43:15.344072Z","shell.execute_reply":"2023-04-03T12:43:15.405408Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.filter(\"type != ''\")","metadata":{"papermill":{"duration":0.15364,"end_time":"2023-04-01T02:50:56.742076","exception":false,"start_time":"2023-04-01T02:50:56.588436","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.408068Z","iopub.execute_input":"2023-04-03T12:43:15.408545Z","iopub.status.idle":"2023-04-03T12:43:15.524829Z","shell.execute_reply.started":"2023-04-03T12:43:15.408498Z","shell.execute_reply":"2023-04-03T12:43:15.523119Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tmp_label_col = \"type\"","metadata":{"papermill":{"duration":0.07212,"end_time":"2023-04-01T02:50:56.875824","exception":false,"start_time":"2023-04-01T02:50:56.803704","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.526144Z","iopub.execute_input":"2023-04-03T12:43:15.526635Z","iopub.status.idle":"2023-04-03T12:43:15.533624Z","shell.execute_reply.started":"2023-04-03T12:43:15.526585Z","shell.execute_reply":"2023-04-03T12:43:15.531984Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features_col = list(df.columns)\nfeatures_col.remove(tmp_label_col)","metadata":{"papermill":{"duration":0.118242,"end_time":"2023-04-01T02:50:57.056577","exception":false,"start_time":"2023-04-01T02:50:56.938335","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.535724Z","iopub.execute_input":"2023-04-03T12:43:15.537194Z","iopub.status.idle":"2023-04-03T12:43:15.593177Z","shell.execute_reply.started":"2023-04-03T12:43:15.537049Z","shell.execute_reply":"2023-04-03T12:43:15.590446Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, _ = df.randomSplit([.8, .2],seed=2)\n","metadata":{"papermill":{"duration":0.311738,"end_time":"2023-04-01T02:50:57.434785","exception":false,"start_time":"2023-04-01T02:50:57.123047","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.600933Z","iopub.execute_input":"2023-04-03T12:43:15.604621Z","iopub.status.idle":"2023-04-03T12:43:15.704375Z","shell.execute_reply.started":"2023-04-03T12:43:15.604545Z","shell.execute_reply":"2023-04-03T12:43:15.70308Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.limit(5).toPandas()","metadata":{"papermill":{"duration":11.893869,"end_time":"2023-04-01T02:51:09.386671","exception":false,"start_time":"2023-04-01T02:50:57.492802","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:15.705715Z","iopub.execute_input":"2023-04-03T12:43:15.706155Z","iopub.status.idle":"2023-04-03T12:43:26.578048Z","shell.execute_reply.started":"2023-04-03T12:43:15.706108Z","shell.execute_reply":"2023-04-03T12:43:26.57692Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_col = \"selectedFeatures\"\nlabel_col = \"label_index\"","metadata":{"papermill":{"duration":0.051277,"end_time":"2023-04-01T02:51:09.480255","exception":false,"start_time":"2023-04-01T02:51:09.428978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:26.579383Z","iopub.execute_input":"2023-04-03T12:43:26.57971Z","iopub.status.idle":"2023-04-03T12:43:26.585903Z","shell.execute_reply.started":"2023-04-03T12:43:26.579681Z","shell.execute_reply":"2023-04-03T12:43:26.584653Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preprocessing_stages = []\n\nfrom pyspark.ml.feature import StandardScaler, VectorAssembler\n\nunscaled_assembler = VectorAssembler(inputCols=features_col, outputCol=\"unscaled_features\")\nscaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\nassembler = VectorAssembler(inputCols=[\"scaled_features\"], outputCol=\"features\") \n\npreprocessing_stages += [unscaled_assembler, scaler, assembler]","metadata":{"papermill":{"duration":0.463376,"end_time":"2023-04-01T02:51:09.982885","exception":false,"start_time":"2023-04-01T02:51:09.519509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:26.588746Z","iopub.execute_input":"2023-04-03T12:43:26.589141Z","iopub.status.idle":"2023-04-03T12:43:27.145172Z","shell.execute_reply.started":"2023-04-03T12:43:26.589101Z","shell.execute_reply":"2023-04-03T12:43:27.144083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer \nfrom pyspark.ml.feature import OneHotEncoder\n\n\nlabel_str_index =  StringIndexer(inputCol=tmp_label_col, outputCol=label_col)\n\npreprocessing_stages += [label_str_index]","metadata":{"papermill":{"duration":0.062947,"end_time":"2023-04-01T02:51:10.086611","exception":false,"start_time":"2023-04-01T02:51:10.023664","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:27.146262Z","iopub.execute_input":"2023-04-03T12:43:27.146658Z","iopub.status.idle":"2023-04-03T12:43:27.164866Z","shell.execute_reply.started":"2023-04-03T12:43:27.146624Z","shell.execute_reply":"2023-04-03T12:43:27.163839Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.feature import UnivariateFeatureSelector\n\nselector = UnivariateFeatureSelector(featuresCol=\"features\", outputCol=feature_col,\n                                     labelCol=\"label_index\", selectionMode=\"numTopFeatures\")\n\nselector.setFeatureType(\"continuous\").setLabelType(\"continuous\").setSelectionThreshold(60)\n\npreprocessing_stages += [selector]","metadata":{"papermill":{"duration":0.060396,"end_time":"2023-04-01T02:51:10.186388","exception":false,"start_time":"2023-04-01T02:51:10.125992","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:27.165874Z","iopub.execute_input":"2023-04-03T12:43:27.16621Z","iopub.status.idle":"2023-04-03T12:43:27.181605Z","shell.execute_reply.started":"2023-04-03T12:43:27.16618Z","shell.execute_reply":"2023-04-03T12:43:27.180329Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml import Pipeline\n\n# Set Pipeline\npreprocessing_pipeline = Pipeline(stages=preprocessing_stages)\n\n# Fit Pipeline to Data\npreprocessing_pipeline = preprocessing_pipeline.fit(df)\n\n# Transform Data using Fitted Pipeline\ntrain_df = preprocessing_pipeline.transform(df)","metadata":{"papermill":{"duration":856.867876,"end_time":"2023-04-01T03:05:27.093538","exception":false,"start_time":"2023-04-01T02:51:10.225662","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:43:27.183075Z","iopub.execute_input":"2023-04-03T12:43:27.183446Z","iopub.status.idle":"2023-04-03T12:56:39.580158Z","shell.execute_reply.started":"2023-04-03T12:43:27.183412Z","shell.execute_reply":"2023-04-03T12:56:39.578447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of Classes\nnb_classes = train_df.select(\"label_index\").distinct().count()\n\n# Number of Inputs or Input Dimensions\ninput_dim = len(train_df.select(feature_col).first()[0])\nprint(\"nb_classes: \", nb_classes)\nprint(\"input_dim: \", input_dim)","metadata":{"papermill":{"duration":159.002773,"end_time":"2023-04-01T03:08:06.151488","exception":false,"start_time":"2023-04-01T03:05:27.148715","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:56:39.584908Z","iopub.execute_input":"2023-04-03T12:56:39.585378Z","iopub.status.idle":"2023-04-03T12:59:07.671368Z","shell.execute_reply.started":"2023-04-03T12:56:39.585333Z","shell.execute_reply":"2023-04-03T12:59:07.670224Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import col\n\ny = train_df.select(col(\"label_index\")).toPandas()\ny = y[\"label_index\"].astype(int)","metadata":{"papermill":{"duration":190.369719,"end_time":"2023-04-01T03:11:16.705618","exception":false,"start_time":"2023-04-01T03:08:06.335899","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T12:59:07.679116Z","iopub.execute_input":"2023-04-03T12:59:07.679543Z","iopub.status.idle":"2023-04-03T13:02:08.048625Z","shell.execute_reply.started":"2023-04-03T12:59:07.679504Z","shell.execute_reply":"2023-04-03T13:02:08.047501Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils import class_weight\n\n\ny_collect = train_df.select(\"label_index\").groupBy(\"label_index\").count().collect()\nunique_y = [x[\"label_index\"] for x in y_collect]\ntotal_y = sum([x[\"count\"] for x in y_collect])\nunique_y_count = len(y_collect)\nbin_count = [x[\"count\"] for x in y_collect]\n\nclass_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}\nclass_weights_spark = [class_weights_spark[key] for key in sorted(class_weights_spark.keys())]\nclass_weights_spark\n\n# class_weights = class_weight.compute_class_weight('balanced',\n#                                                   classes=np.unique(y),\n#                                                   y=y)\n\n# class_weights","metadata":{"papermill":{"duration":157.959781,"end_time":"2023-04-01T03:13:54.732062","exception":false,"start_time":"2023-04-01T03:11:16.772281","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:02:08.050012Z","iopub.execute_input":"2023-04-03T13:02:08.050502Z","iopub.status.idle":"2023-04-03T13:04:38.68577Z","shell.execute_reply.started":"2023-04-03T13:02:08.050466Z","shell.execute_reply":"2023-04-03T13:04:38.684385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.0723,"end_time":"2023-04-01T03:13:54.902263","exception":false,"start_time":"2023-04-01T03:13:54.829963","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.077951,"end_time":"2023-04-01T03:13:55.052633","exception":false,"start_time":"2023-04-01T03:13:54.974682","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_files = glob.glob(\"/kaggle/input/nbaiot-dataset/*.csv\")\n[csv_files.remove(f) for f in out_files];","metadata":{"papermill":{"duration":0.088379,"end_time":"2023-04-01T03:13:55.214517","exception":false,"start_time":"2023-04-01T03:13:55.126138","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:04:38.68777Z","iopub.execute_input":"2023-04-03T13:04:38.68967Z","iopub.status.idle":"2023-04-03T13:04:38.698718Z","shell.execute_reply.started":"2023-04-03T13:04:38.689617Z","shell.execute_reply":"2023-04-03T13:04:38.69737Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from  pyspark.sql.functions import input_file_name\nfrom pyspark.sql.functions import col, udf\n\ndef get_client_csvs(pack_num):\n    pack_csv_file = []\n    for f in csv_files:\n        if pack_num in f:\n            pack_csv_file.append(f)  \n    return pack_csv_file\n\ndef read_csv(pack_csv_file, schema):\n    return spark.read.csv(pack_csv_file, header=False, schema=schema)\n\ndef getType(path):\n    sampleType = path.split('/')[-1].split('.')[1:-1]\n    return \"_\".join(sampleType)\n\ndef df_preparation(df):\n    df = df.na.drop()\n    df = df.withColumn(\"path\", input_file_name())\n    getTypeUDF = udf(lambda x:getType(x),StringType()) \n    df = df.withColumn(\"type\", getTypeUDF(col(\"path\")))\n    df = df.drop(\"path\")\n    df = df.filter(\"type != ''\")\n    return df\n    \ndef read_client_df(client_name, schema):\n    pack_num = client_name.split('_')[-1]\n    pack_csv_file = get_client_csvs(pack_num)\n    df = read_csv(pack_csv_file, schema)\n    return df_preparation(df)\n\n\ndef df_preprocessing(df, preprocessing_pipeline):\n    transformed_df = preprocessing_pipeline.transform(df)\n    return transformed_df.select(feature_col, label_col)","metadata":{"papermill":{"duration":0.096195,"end_time":"2023-04-01T03:13:55.381576","exception":false,"start_time":"2023-04-01T03:13:55.285381","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:04:38.701067Z","iopub.execute_input":"2023-04-03T13:04:38.702079Z","iopub.status.idle":"2023-04-03T13:04:38.717699Z","shell.execute_reply.started":"2023-04-03T13:04:38.702024Z","shell.execute_reply":"2023-04-03T13:04:38.7164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clients = {}\nnum_clients = 9\nfor n in range(num_clients):\n    clients[\"client_{}\".format(n+1)]=None\nclients","metadata":{"papermill":{"duration":0.084689,"end_time":"2023-04-01T03:13:55.541798","exception":false,"start_time":"2023-04-01T03:13:55.457109","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:04:38.719784Z","iopub.execute_input":"2023-04-03T13:04:38.720703Z","iopub.status.idle":"2023-04-03T13:04:38.734383Z","shell.execute_reply.started":"2023-04-03T13:04:38.720651Z","shell.execute_reply":"2023-04-03T13:04:38.733088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k in clients.keys():\n    clients[k] = {}\n    client_df = read_client_df(k, schema)\n    clients[k][\"df\"] = client_df\n    \nfull_dataset_size = sum([clients[c_key][\"df\"].count() for c_key in clients.keys()])\nfull_dataset_size","metadata":{"papermill":{"duration":154.523802,"end_time":"2023-04-01T03:16:30.132755","exception":false,"start_time":"2023-04-01T03:13:55.608953","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:04:38.736522Z","iopub.execute_input":"2023-04-03T13:04:38.737398Z","iopub.status.idle":"2023-04-03T13:07:50.713035Z","shell.execute_reply.started":"2023-04-03T13:04:38.737344Z","shell.execute_reply":"2023-04-03T13:07:50.711397Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.142933,"end_time":"2023-04-01T03:16:30.358601","exception":false,"start_time":"2023-04-01T03:16:30.215668","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.141046,"end_time":"2023-04-01T03:16:30.641769","exception":false,"start_time":"2023-04-01T03:16:30.500723","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for c_key in clients.keys():\n    clients[c_key][\"df\"] = df_preprocessing(clients[c_key][\"df\"], preprocessing_pipeline)\n\nhyper_params = {\n    \"full_dataset_size\":full_dataset_size,\n    \"class_weights_spark\": class_weights_spark,\n    \"nb_classes\":  nb_classes,\n    \"input_dim\":  input_dim\n}","metadata":{"papermill":{"duration":2.559039,"end_time":"2023-04-01T03:16:33.341418","exception":false,"start_time":"2023-04-01T03:16:30.782379","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:07:50.716546Z","iopub.execute_input":"2023-04-03T13:07:50.716924Z","iopub.status.idle":"2023-04-03T13:07:52.881115Z","shell.execute_reply.started":"2023-04-03T13:07:50.716889Z","shell.execute_reply":"2023-04-03T13:07:52.879905Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving Previous pipeline and params for not running it from beginning of connection reset","metadata":{"papermill":{"duration":0.079601,"end_time":"2023-04-01T03:16:33.499046","exception":false,"start_time":"2023-04-01T03:16:33.419445","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pickle\n\npreprocessing_pipeline.write().overwrite().save(\"preprocessing_pipeline\")\n\nfor c_key in clients.keys():\n    clients[c_key][\"df\"].write.parquet(\"preprocessed_df_{}.parquet\".format(c_key))\n    \nwith open('hyper_params.pkl', 'wb') as fp:\n    pickle.dump(hyper_params, fp)","metadata":{"papermill":{"duration":347.205168,"end_time":"2023-04-01T03:22:20.78559","exception":false,"start_time":"2023-04-01T03:16:33.580422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:07:52.882496Z","iopub.execute_input":"2023-04-03T13:07:52.882963Z","iopub.status.idle":"2023-04-03T13:12:38.392478Z","shell.execute_reply.started":"2023-04-03T13:07:52.882928Z","shell.execute_reply":"2023-04-03T13:12:38.391058Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nfrom pyspark.ml import PipelineModel\n\npreprocessing_pipeline = PipelineModel.load('./preprocessing_pipeline')\n\nfor c_key in clients.keys():\n    clients[c_key][\"df\"] = spark.read.parquet(\"preprocessed_df_{}.parquet\".format(c_key))\n    \n\nwith open('hyper_params.pkl', 'rb') as fp:\n    hyper_params = pickle.load(fp)\n    \n\nfull_dataset_size = hyper_params[\"full_dataset_size\"]\nclass_weights = hyper_params[\"class_weights_spark\"]\nnb_classes = hyper_params[\"nb_classes\"]\ninput_dim = hyper_params[\"input_dim\"]","metadata":{"papermill":{"duration":4.232811,"end_time":"2023-04-01T03:22:25.096121","exception":false,"start_time":"2023-04-01T03:22:20.86331","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:38.393976Z","iopub.execute_input":"2023-04-03T13:12:38.394491Z","iopub.status.idle":"2023-04-03T13:12:42.043586Z","shell.execute_reply.started":"2023-04-03T13:12:38.394445Z","shell.execute_reply":"2023-04-03T13:12:42.042231Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.078773,"end_time":"2023-04-01T03:22:25.264429","exception":false,"start_time":"2023-04-01T03:22:25.185656","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.081145,"end_time":"2023-04-01T03:22:25.424822","exception":false,"start_time":"2023-04-01T03:22:25.343677","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keras / Deep Learning\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam, RMSprop","metadata":{"papermill":{"duration":9.069564,"end_time":"2023-04-01T03:22:34.56994","exception":false,"start_time":"2023-04-01T03:22:25.500376","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:42.048209Z","iopub.execute_input":"2023-04-03T13:12:42.048736Z","iopub.status.idle":"2023-04-03T13:12:50.624244Z","shell.execute_reply.started":"2023-04-03T13:12:42.048684Z","shell.execute_reply":"2023-04-03T13:12:50.622614Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.metrics import Recall, Precision\n\n\ndef f1_score(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val\n\n","metadata":{"papermill":{"duration":0.091004,"end_time":"2023-04-01T03:22:34.740388","exception":false,"start_time":"2023-04-01T03:22:34.649384","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:50.626023Z","iopub.execute_input":"2023-04-03T13:12:50.626815Z","iopub.status.idle":"2023-04-03T13:12:50.636372Z","shell.execute_reply.started":"2023-04-03T13:12:50.62677Z","shell.execute_reply":"2023-04-03T13:12:50.63476Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras import backend as K\n\ndef weighted_categorical_crossentropy(weights):\n    \n    weights = K.variable(weights)\n        \n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    \n    return loss","metadata":{"papermill":{"duration":0.094861,"end_time":"2023-04-01T03:22:34.915572","exception":false,"start_time":"2023-04-01T03:22:34.820711","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:50.637796Z","iopub.execute_input":"2023-04-03T13:12:50.638139Z","iopub.status.idle":"2023-04-03T13:12:50.654644Z","shell.execute_reply.started":"2023-04-03T13:12:50.638108Z","shell.execute_reply":"2023-04-03T13:12:50.653521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up Deep Learning Model / Architecture\n\nclass CustomModel:\n    @staticmethod\n    def build(input_dim, num_classes):            \n        model = Sequential()\n        model.add(Dense(64, input_shape=(input_dim,), activation=\"relu\"))\n        model.add(Dense(32, activation=\"relu\"))\n        model.add(Dropout(rate=0.2))\n        model.add(Dense(nb_classes, activation=\"softmax\"))\n        return model","metadata":{"papermill":{"duration":0.088211,"end_time":"2023-04-01T03:22:35.08132","exception":false,"start_time":"2023-04-01T03:22:34.993109","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:50.656551Z","iopub.execute_input":"2023-04-03T13:12:50.657847Z","iopub.status.idle":"2023-04-03T13:12:50.668341Z","shell.execute_reply.started":"2023-04-03T13:12:50.657797Z","shell.execute_reply":"2023-04-03T13:12:50.667387Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.metrics import Recall, Precision\nfrom keras.utils.vis_utils import plot_model\n\ninit_learning_rate = 0.001\nlearning_rate = init_learning_rate \ncomms_round = 30\nbatch_size = 64\nloss=weighted_categorical_crossentropy(class_weights)\nmetrics = [\"accuracy\", Recall(), Precision(), f1_score]\noptimizer = Adam(learning_rate=learning_rate)\n\n#initialize global model\nglobal_model = CustomModel()\nglobal_model = global_model.build(input_dim, nb_classes)\n\nglobal_model.summary()\n","metadata":{"papermill":{"duration":0.443038,"end_time":"2023-04-01T03:22:35.601807","exception":false,"start_time":"2023-04-01T03:22:35.158769","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:50.669501Z","iopub.execute_input":"2023-04-03T13:12:50.670912Z","iopub.status.idle":"2023-04-03T13:12:51.038071Z","shell.execute_reply.started":"2023-04-03T13:12:50.670858Z","shell.execute_reply":"2023-04-03T13:12:51.037169Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n\nplot_model(global_model, to_file='global_model.png', show_shapes=True, show_layer_names=True)","metadata":{"papermill":{"duration":1.292651,"end_time":"2023-04-01T03:22:36.973379","exception":false,"start_time":"2023-04-01T03:22:35.680728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:51.039109Z","iopub.execute_input":"2023-04-03T13:12:51.040093Z","iopub.status.idle":"2023-04-03T13:12:52.208642Z","shell.execute_reply.started":"2023-04-03T13:12:51.040054Z","shell.execute_reply":"2023-04-03T13:12:52.20715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def weight_scalling_factor(full_dataset_size, client_dataset_size):\n    return client_dataset_size/full_dataset_size","metadata":{"papermill":{"duration":0.088051,"end_time":"2023-04-01T03:22:37.139361","exception":false,"start_time":"2023-04-01T03:22:37.05131","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.211233Z","iopub.execute_input":"2023-04-03T13:12:52.2117Z","iopub.status.idle":"2023-04-03T13:12:52.217774Z","shell.execute_reply.started":"2023-04-03T13:12:52.211658Z","shell.execute_reply":"2023-04-03T13:12:52.216191Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def scale_model_weights(weight, scalar):\n    '''function for scaling a models weights'''\n    weight_final = []\n    steps = len(weight)\n    for i in range(steps):\n        weight_final.append(scalar * weight[i])\n    return weight_final","metadata":{"papermill":{"duration":0.08912,"end_time":"2023-04-01T03:22:37.305877","exception":false,"start_time":"2023-04-01T03:22:37.216757","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.219691Z","iopub.execute_input":"2023-04-03T13:12:52.221083Z","iopub.status.idle":"2023-04-03T13:12:52.230469Z","shell.execute_reply.started":"2023-04-03T13:12:52.221024Z","shell.execute_reply":"2023-04-03T13:12:52.228909Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sum_scaled_weights(scaled_weight_list):\n    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n    avg_grad = list()\n    #get the average grad accross all client gradients\n    for grad_list_tuple in zip(*scaled_weight_list):\n        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n        avg_grad.append(layer_mean)\n        \n    return avg_grad","metadata":{"papermill":{"duration":0.088401,"end_time":"2023-04-01T03:22:37.471826","exception":false,"start_time":"2023-04-01T03:22:37.383425","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.232001Z","iopub.execute_input":"2023-04-03T13:12:52.232838Z","iopub.status.idle":"2023-04-03T13:12:52.242172Z","shell.execute_reply.started":"2023-04-03T13:12:52.232771Z","shell.execute_reply":"2023-04-03T13:12:52.240928Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef test_model(X_test, y_test,  model, comm_round):\n    #logits = model.predict(X_test, batch_size=100)\n    logits = model.predict(X_test)\n    loss = cce(y_test, logits)\n    y_hat = np.argmax(logits, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n\n    accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(logits, axis=1))\n    \n    r = Recall()\n    r.update_state(y_test, logits)\n    recall = r.result().numpy()\n    \n    p = Precision()\n    p.update_state(y_test, logits)\n    precision = p.result().numpy()\n    \n    f = f1_score(y_test, logits)\n    f1 = f.numpy()\n    \n    print('comm_round: {} | global_loss: {} | global_accuracy: {:.4} | global_recall: {:.4} | global_precision: {:.4} | global_f1_score: {:.4} \\n'.format(comm_round, loss, accuracy, recall, precision, f1))\n    return loss, accuracy, precision, recall, f1","metadata":{"papermill":{"duration":0.16534,"end_time":"2023-04-01T03:22:37.714433","exception":false,"start_time":"2023-04-01T03:22:37.549093","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.243478Z","iopub.execute_input":"2023-04-03T13:12:52.243813Z","iopub.status.idle":"2023-04-03T13:12:52.344003Z","shell.execute_reply.started":"2023-04-03T13:12:52.243783Z","shell.execute_reply":"2023-04-03T13:12:52.342855Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.083533,"end_time":"2023-04-01T03:22:38.053944","exception":false,"start_time":"2023-04-01T03:22:37.970411","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Elephas for Deep Learning on Spark\nfrom elephas.ml_model import ElephasEstimator\nfrom tensorflow.keras import optimizers\n\ndef get_optimizer(lr):\n    # Set and Serialize Optimizer\n    optimizer_conf = optimizers.Adam(learning_rate=lr)\n    opt_conf = optimizers.serialize(optimizer_conf)\n    return opt_conf\n    \n\ndef get_sparkML_estimator(model, nb_classes, opt_conf, class_weights):\n    # Initialize SparkML Estimator and Get Settings\n    estimator = ElephasEstimator()\n    estimator.setFeaturesCol(feature_col)\n    estimator.setLabelCol(label_col)\n    estimator.set_keras_model_config(model.to_json())\n    estimator.set_categorical_labels(True)\n    estimator.set_nb_classes(nb_classes)\n    estimator.set_num_workers(1)\n    estimator.set_epochs(1) \n    estimator.set_batch_size(batch_size)\n    estimator.set_verbosity(1)\n    estimator.set_optimizer_config(opt_conf)\n    estimator.set_mode(\"synchronous\")\n    estimator.set_num_workers(1)\n    estimator.set_frequency(\"epoch\")\n    estimator.set_validation_split(0.0)\n    estimator.set_loss(weighted_categorical_crossentropy(class_weights))\n    estimator.set_metrics(['accuracy', Recall(), Precision(), f1_score])\n    return estimator","metadata":{"papermill":{"duration":0.300036,"end_time":"2023-04-01T03:22:38.437211","exception":false,"start_time":"2023-04-01T03:22:38.137175","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.345726Z","iopub.execute_input":"2023-04-03T13:12:52.346199Z","iopub.status.idle":"2023-04-03T13:12:52.555752Z","shell.execute_reply.started":"2023-04-03T13:12:52.346158Z","shell.execute_reply":"2023-04-03T13:12:52.554643Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We can add the preprocessing pipeline stages with the estimator as a single pipeline for whole process\ndef get_pipeline_model(estimator):\n    stages = [estimator]\n    return Pipeline(stages=stages)\n    ","metadata":{"papermill":{"duration":0.091913,"end_time":"2023-04-01T03:22:38.609422","exception":false,"start_time":"2023-04-01T03:22:38.517509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.557418Z","iopub.execute_input":"2023-04-03T13:12:52.558231Z","iopub.status.idle":"2023-04-03T13:12:52.565194Z","shell.execute_reply.started":"2023-04-03T13:12:52.558179Z","shell.execute_reply":"2023-04-03T13:12:52.563815Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml import Pipeline\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nfrom elephas.ml_model import ElephasTransformer\nfrom elephas.utils.model_utils import ModelType, argmax\n\n\ndef fit_pipeline_model(pipeline, train_df):\n    return pipeline.fit(train_df)\n\ndef train_client(global_weights, client_df, learning_rate):\n    local_model = CustomModel()\n    local_model = local_model.build(input_dim, nb_classes)\n\n    #set local model weight to the weight of the global model\n    local_model.set_weights(global_weights)\n\n    #fit local model with client's data\n    opt_conf = get_optimizer(learning_rate)\n    estimator = get_sparkML_estimator(local_model, nb_classes, opt_conf, class_weights)\n    pipeline = get_pipeline_model(estimator)\n    \n    fitted_pipeline = fit_pipeline_model(pipeline, client_df)\n\n    client_dataset_size = client_df.count()\n    local_model_weights = estimator.get_model().get_weights()\n    #scale the model weights and add to list\n    scaling_factor = weight_scalling_factor(full_dataset_size, client_dataset_size)\n    scaled_weights = scale_model_weights(local_model_weights, scaling_factor)\n    \n    return fitted_pipeline, scaled_weights\n","metadata":{"papermill":{"duration":0.098019,"end_time":"2023-04-01T03:22:38.790989","exception":false,"start_time":"2023-04-01T03:22:38.69297","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.567287Z","iopub.execute_input":"2023-04-03T13:12:52.567841Z","iopub.status.idle":"2023-04-03T13:12:52.580592Z","shell.execute_reply.started":"2023-04-03T13:12:52.5678Z","shell.execute_reply":"2023-04-03T13:12:52.579393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def learning_rate_scheduler(nb_round, learning_rate):\n    if (nb_round > 0 and nb_round % 2):\n        return learning_rate * 0.5\n    return learning_rate","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:12:52.582262Z","iopub.execute_input":"2023-04-03T13:12:52.583434Z","iopub.status.idle":"2023-04-03T13:12:52.594659Z","shell.execute_reply.started":"2023-04-03T13:12:52.583383Z","shell.execute_reply":"2023-04-03T13:12:52.593412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom elephas.spark_model import SparkModel\n\n\n#commence global training loop\nglobal_accuracy=[]\nglobal_precision=[]\nglobal_recall=[]\nglobal_f1=[]\nbest_global_accuracy = 0\n    \nnumber_rounds = 10\nlearning_rate = 0.001\n\nglobal_pipeline = None\nlocal_pipeline = None\n\nfor comm_round in range(number_rounds): \n    learning_rate = learning_rate_scheduler(comm_round, learning_rate)\n\n    # get the global model's weights - will serve as the initial weights for all local models\n    global_weights = global_model.get_weights()\n    \n    #initial list to collect local model weights after scalling\n    scaled_local_weight_list = list()\n\n    #randomize client data - using keys\n    client_names= list(clients.keys())[:-1]\n    random.shuffle(client_names)\n            \n    #loop through each client and create new local model\n    for c_key in client_names:  \n        \n        print(f\"Round: {comm_round} | Client: {c_key} training\")\n        client_df = clients[c_key][\"df\"]\n        local_pipeline, scaled_weights = train_client(global_weights, client_df, learning_rate)\n        scaled_local_weight_list.append(scaled_weights)\n        global_pipeline = local_pipeline\n            \n    #to get the average over all the local model, we simply take the sum of the scaled weights\n    average_weights = sum_scaled_weights(scaled_local_weight_list)\n    \n    #update global model \n    global_model.set_weights(average_weights)\n    global_pipeline.stages[-1].get_model().set_weights(average_weights)\n    \n    #test global model and print out metrics after each communications round\n    test_client_key = list(clients.keys())[-1]\n    test_df = clients[test_client_key][\"df\"]\n    prediction_and_label = global_pipeline.transform(test_df)\n\n    prediction_and_label = prediction_and_label.select(label_col, argmax('prediction').astype(DoubleType()).alias('prediction'))\n    prediction_and_label = prediction_and_label.rdd.map(lambda row: (row[label_col], row[\"prediction\"]))\n    metrics = MulticlassMetrics(prediction_and_label)\n    \n    print(\"round {} results on client '{}'\".format(comm_round, test_client_key))\n    g_accuracy = metrics.accuracy\n    weightedPrecision = metrics.weightedPrecision\n    weightedRecall = metrics.weightedRecall\n    weightedF1Score = 2*((weightedPrecision*weightedRecall)/(weightedPrecision+weightedRecall))\n    print(\"global accuracy : \", g_accuracy)\n    print(\"global weightedPrecision : \",weightedPrecision)\n    print(\"global weightedRecall : \",weightedRecall)\n    print(\"global weightedF1Score : \",weightedF1Score)\n    \n    global_accuracy.append(g_accuracy)\n    global_precision.append(weightedPrecision)\n    global_recall.append(weightedRecall)\n    global_f1.append(weightedF1Score)\n        \n    if g_accuracy > best_global_accuracy:\n        best_global_accuracy = g_accuracy\n        global_pipeline.stages[-1].save('global_transformer_best_weights.h5')\n        print(\"New Weights Saved\")","metadata":{"papermill":{"duration":11327.228759,"end_time":"2023-04-01T06:31:26.097612","exception":false,"start_time":"2023-04-01T03:22:38.868853","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:12:52.596682Z","iopub.execute_input":"2023-04-03T13:12:52.597057Z","iopub.status.idle":"2023-04-03T13:19:10.641788Z","shell.execute_reply.started":"2023-04-03T13:12:52.597022Z","shell.execute_reply":"2023-04-03T13:19:10.64027Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from elephas.ml_model import load_ml_transformer\n\nload_ml_transformer = load_ml_transformer(\"global_transformer_best_weights.h5\")\nprediction_and_label = load_ml_transformer.transform(test_df)\n\nprediction_and_label.head(5)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-03T13:23:43.142384Z","iopub.execute_input":"2023-04-03T13:23:43.142794Z","iopub.status.idle":"2023-04-03T13:24:13.355014Z","shell.execute_reply.started":"2023-04-03T13:23:43.142762Z","shell.execute_reply":"2023-04-03T13:24:13.353565Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nfigure, axis = plt.subplots(2, 2, figsize=(15, 15))\n\naxis[0, 0].plot(global_accuracy)\naxis[0, 0].set_title(\"global accuracy\")\n  \naxis[0, 1].plot(global_precision)\naxis[0, 1].set_title(\"global weighted Precision\")\n  \naxis[1, 0].plot(global_recall)\naxis[1, 0].set_title(\"global weighted Recall\")\n\naxis[1, 1].plot(global_f1)\naxis[1, 1].set_title(\"global weighted F1Score\")\n  \nplt.show()\n","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2023-04-03T13:19:11.163817Z","iopub.status.idle":"2023-04-03T13:19:11.164309Z","shell.execute_reply.started":"2023-04-03T13:19:11.164081Z","shell.execute_reply":"2023-04-03T13:19:11.164101Z"},"trusted":true},"outputs":[],"execution_count":null}]}