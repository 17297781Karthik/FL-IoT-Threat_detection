import osimport pandas as pdimport numpy as npfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.model_selection import train_test_splitimport torchfrom torch.utils.data import DataLoader, TensorDatasetdef prepare_data_pipeline(device_type, data_dir='fdata', test_size=0.2, batch_size=1024, random_state=42):    """    Prepares the data pipeline for a specific device type.    Args:        device_type (int): The device type (e.g., 1 for the first device).        data_dir (str): Directory containing the CSV files.        test_size (float): Proportion of the data to use for testing.        batch_size (int): Batch size for DataLoaders.        random_state (int): Random seed for reproducibility.    Returns:        train_loader (DataLoader): DataLoader for training data.        test_loader (DataLoader): DataLoader for testing data.        input_size (int): Number of input features.        num_classes (int): Number of unique classes.    """    # Define attack labels    att_label = {        f'{device_type}.gafgyt.combo.csv': 1,        f'{device_type}.gafgyt.junk.csv': 2,        f'{device_type}.gafgyt.tcp.csv': 3,        f'{device_type}.gafgyt.udp.csv': 4,        f'{device_type}.mirai.ack.csv': 5,        f'{device_type}.mirai.scan.csv': 6,        f'{device_type}.mirai.syn.csv': 7,        f'{device_type}.mirai.udp.csv': 8,        f'{device_type}.mirai.udpplain.csv': 9    }    # Load benign data    benign_file = f'{data_dir}/{device_type}.benign.csv'    beg1 = pd.read_csv(benign_file)    beg1['label'] = 0  # Label benign data as 0    # Load attack data    attack_files = [f for f in os.listdir(data_dir) if                    f.startswith(f'{device_type}.') and f != f'{device_type}.gafgyt.scan.csv']    att = pd.DataFrame(columns=beg1.columns)    for file in attack_files:        data = pd.read_csv(f'{data_dir}/{file}')        data['label'] = att_label[file]        att = pd.concat([att, data], ignore_index=True)    # Combine benign and attack data    att = pd.concat([att, beg1], ignore_index=True)    # Shuffle data    att = att.sample(frac=1, random_state=random_state).reset_index(drop=True)    # Separate features and labels    X = att.drop(columns=['label'])    y = att['label']    # Scale numerical features    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns    scaler = MinMaxScaler()    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])    # Train-test split    X_train, X_test, y_train, y_test = train_test_split(        X, y, test_size=test_size, stratify=y, random_state=random_state    )    # Convert to tensors    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)    # Create DataLoaders    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)    # Return DataLoaders and metadata    input_size = X_train.shape[1]    num_classes = len(y.unique())    return train_loader, test_loader, input_size, num_classes