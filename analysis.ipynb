{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:08.812806Z",
     "start_time": "2025-07-18T05:18:08.806643Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6487086469ca498c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:11.345146Z",
     "start_time": "2025-07-18T05:18:11.333283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.benign.csv\n",
      "1.gafgyt.combo.csv\n",
      "1.gafgyt.junk.csv\n",
      "1.gafgyt.scan.csv\n",
      "1.gafgyt.tcp.csv\n",
      "1.gafgyt.udp.csv\n",
      "1.mirai.ack.csv\n",
      "1.mirai.scan.csv\n",
      "1.mirai.syn.csv\n",
      "1.mirai.udp.csv\n",
      "1.mirai.udpplain.csv\n",
      "2.benign.csv\n",
      "2.gafgyt.combo.csv\n",
      "2.gafgyt.junk.csv\n",
      "2.gafgyt.scan.csv\n",
      "2.gafgyt.tcp.csv\n",
      "2.gafgyt.udp.csv\n",
      "2.mirai.ack.csv\n",
      "2.mirai.scan.csv\n",
      "2.mirai.syn.csv\n",
      "2.mirai.udp.csv\n",
      "2.mirai.udpplain.csv\n",
      "3.benign.csv\n",
      "3.gafgyt.combo.csv\n",
      "3.gafgyt.junk.csv\n",
      "3.gafgyt.scan.csv\n",
      "3.gafgyt.tcp.csv\n",
      "3.gafgyt.udp.csv\n",
      "4.benign.csv\n",
      "4.gafgyt.combo.csv\n",
      "4.gafgyt.junk.csv\n",
      "4.gafgyt.scan.csv\n",
      "4.gafgyt.tcp.csv\n",
      "4.gafgyt.udp.csv\n",
      "4.mirai.ack.csv\n",
      "4.mirai.scan.csv\n",
      "4.mirai.syn.csv\n",
      "4.mirai.udp.csv\n",
      "4.mirai.udpplain.csv\n",
      "5.benign.csv\n",
      "5.gafgyt.combo.csv\n",
      "5.gafgyt.junk.csv\n",
      "5.gafgyt.scan.csv\n",
      "5.gafgyt.tcp.csv\n",
      "5.gafgyt.udp.csv\n",
      "5.mirai.ack.csv\n",
      "5.mirai.scan.csv\n",
      "5.mirai.syn.csv\n",
      "5.mirai.udp.csv\n",
      "5.mirai.udpplain.csv\n",
      "6.benign.csv\n",
      "6.gafgyt.combo.csv\n",
      "6.gafgyt.junk.csv\n",
      "6.gafgyt.scan.csv\n",
      "6.gafgyt.tcp.csv\n",
      "6.gafgyt.udp.csv\n",
      "6.mirai.ack.csv\n",
      "6.mirai.scan.csv\n",
      "6.mirai.syn.csv\n",
      "6.mirai.udp.csv\n",
      "6.mirai.udpplain.csv\n",
      "7.benign.csv\n",
      "7.gafgyt.combo.csv\n",
      "7.gafgyt.junk.csv\n",
      "7.gafgyt.scan.csv\n",
      "7.gafgyt.tcp.csv\n",
      "7.gafgyt.udp.csv\n",
      "8.benign.csv\n",
      "8.gafgyt.combo.csv\n",
      "8.gafgyt.junk.csv\n",
      "8.gafgyt.scan.csv\n",
      "8.gafgyt.tcp.csv\n",
      "8.gafgyt.udp.csv\n",
      "8.mirai.ack.csv\n",
      "8.mirai.scan.csv\n",
      "8.mirai.syn.csv\n",
      "8.mirai.udp.csv\n",
      "8.mirai.udpplain.csv\n",
      "9.benign.csv\n",
      "9.gafgyt.combo.csv\n",
      "9.gafgyt.junk.csv\n",
      "9.gafgyt.scan.csv\n",
      "9.gafgyt.tcp.csv\n",
      "9.gafgyt.udp.csv\n",
      "9.mirai.ack.csv\n",
      "9.mirai.scan.csv\n",
      "9.mirai.syn.csv\n",
      "9.mirai.udp.csv\n",
      "9.mirai.udpplain.csv\n",
      "data_summary.csv\n",
      "device_info.csv\n",
      "features.csv\n",
      "README.md\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('fdata'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39baf1399d5261d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:15.698626Z",
     "start_time": "2025-07-18T05:18:13.538612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MI_dir_L5_weight', 'MI_dir_L5_mean', 'MI_dir_L5_variance',\n",
       "       'MI_dir_L3_weight', 'MI_dir_L3_mean', 'MI_dir_L3_variance',\n",
       "       'MI_dir_L1_weight', 'MI_dir_L1_mean', 'MI_dir_L1_variance',\n",
       "       'MI_dir_L0.1_weight',\n",
       "       ...\n",
       "       'HpHp_L0.1_radius', 'HpHp_L0.1_covariance', 'HpHp_L0.1_pcc',\n",
       "       'HpHp_L0.01_weight', 'HpHp_L0.01_mean', 'HpHp_L0.01_std',\n",
       "       'HpHp_L0.01_magnitude', 'HpHp_L0.01_radius', 'HpHp_L0.01_covariance',\n",
       "       'HpHp_L0.01_pcc'],\n",
       "      dtype='object', length=115)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd\n",
    "beg1=pd.read_csv('fdata/1.benign.csv')\n",
    "beg1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e50e62b99c49fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:18.105894Z",
     "start_time": "2025-07-18T05:18:18.096907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49548, 115)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beg1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "545dfc80c0fe1b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:18.969203Z",
     "start_time": "2025-07-18T05:18:18.920207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MI_dir_L5_weight</th>\n",
       "      <th>MI_dir_L5_mean</th>\n",
       "      <th>MI_dir_L5_variance</th>\n",
       "      <th>MI_dir_L3_weight</th>\n",
       "      <th>MI_dir_L3_mean</th>\n",
       "      <th>MI_dir_L3_variance</th>\n",
       "      <th>MI_dir_L1_weight</th>\n",
       "      <th>MI_dir_L1_mean</th>\n",
       "      <th>MI_dir_L1_variance</th>\n",
       "      <th>MI_dir_L0.1_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>HpHp_L0.1_radius</th>\n",
       "      <th>HpHp_L0.1_covariance</th>\n",
       "      <th>HpHp_L0.1_pcc</th>\n",
       "      <th>HpHp_L0.01_weight</th>\n",
       "      <th>HpHp_L0.01_mean</th>\n",
       "      <th>HpHp_L0.01_std</th>\n",
       "      <th>HpHp_L0.01_magnitude</th>\n",
       "      <th>HpHp_L0.01_radius</th>\n",
       "      <th>HpHp_L0.01_covariance</th>\n",
       "      <th>HpHp_L0.01_pcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.095047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.319895</td>\n",
       "      <td>344.262695</td>\n",
       "      <td>4.710446</td>\n",
       "      <td>344.262695</td>\n",
       "      <td>22.188299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  MI_dir_L3_weight   \n",
       "0               1.0            60.0                 0.0               1.0  \\\n",
       "1               1.0           354.0                 0.0               1.0   \n",
       "\n",
       "   MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight  MI_dir_L1_mean   \n",
       "0            60.0                 0.0               1.0            60.0  \\\n",
       "1           354.0                 0.0               1.0           354.0   \n",
       "\n",
       "   MI_dir_L1_variance  MI_dir_L0.1_weight  ...  HpHp_L0.1_radius   \n",
       "0                 0.0                 1.0  ...          0.000000  \\\n",
       "1                 0.0                 1.0  ...         34.095047   \n",
       "\n",
       "   HpHp_L0.1_covariance  HpHp_L0.1_pcc  HpHp_L0.01_weight  HpHp_L0.01_mean   \n",
       "0                   0.0            0.0           1.000000        60.000000  \\\n",
       "1                   0.0            0.0           5.319895       344.262695   \n",
       "\n",
       "   HpHp_L0.01_std  HpHp_L0.01_magnitude  HpHp_L0.01_radius   \n",
       "0        0.000000             60.000000           0.000000  \\\n",
       "1        4.710446            344.262695          22.188299   \n",
       "\n",
       "   HpHp_L0.01_covariance  HpHp_L0.01_pcc  \n",
       "0                    0.0             0.0  \n",
       "1                    0.0             0.0  \n",
       "\n",
       "[2 rows x 115 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beg1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5024a0a86acc445b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:20.528591Z",
     "start_time": "2025-07-18T05:18:20.521777Z"
    }
   },
   "outputs": [],
   "source": [
    "lis = os.listdir('fdata')\n",
    "req=[]\n",
    "for i in range(1,11):\n",
    "        req.append(lis[i])\n",
    "req.remove('1.gafgyt.scan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "441130a45faaa56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:21.843743Z",
     "start_time": "2025-07-18T05:18:21.831857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.gafgyt.combo.csv',\n",
       " '1.gafgyt.junk.csv',\n",
       " '1.gafgyt.tcp.csv',\n",
       " '1.gafgyt.udp.csv',\n",
       " '1.mirai.ack.csv',\n",
       " '1.mirai.scan.csv',\n",
       " '1.mirai.syn.csv',\n",
       " '1.mirai.udp.csv',\n",
       " '1.mirai.udpplain.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1c35ee3dad4da3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:23.925928Z",
     "start_time": "2025-07-18T05:18:23.915461Z"
    }
   },
   "outputs": [],
   "source": [
    "col=[i for i in beg1.columns]\n",
    "\n",
    "\n",
    "att=pd.DataFrame(columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b14175dcdf45b885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:24.917049Z",
     "start_time": "2025-07-18T05:18:24.900641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MI_dir_L5_weight', 'MI_dir_L5_mean', 'MI_dir_L5_variance',\n",
       "       'MI_dir_L3_weight', 'MI_dir_L3_mean', 'MI_dir_L3_variance',\n",
       "       'MI_dir_L1_weight', 'MI_dir_L1_mean', 'MI_dir_L1_variance',\n",
       "       'MI_dir_L0.1_weight',\n",
       "       ...\n",
       "       'HpHp_L0.1_radius', 'HpHp_L0.1_covariance', 'HpHp_L0.1_pcc',\n",
       "       'HpHp_L0.01_weight', 'HpHp_L0.01_mean', 'HpHp_L0.01_std',\n",
       "       'HpHp_L0.01_magnitude', 'HpHp_L0.01_radius', 'HpHp_L0.01_covariance',\n",
       "       'HpHp_L0.01_pcc'],\n",
       "      dtype='object', length=115)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_label = {\n",
    "  '1.benign.csv':0,\n",
    "    '1.gafgyt.combo.csv':1,\n",
    " '1.gafgyt.junk.csv':2,\n",
    " '1.gafgyt.tcp.csv':3,\n",
    " '1.gafgyt.udp.csv':4,\n",
    " '1.mirai.ack.csv':5,\n",
    " '1.mirai.scan.csv':6,\n",
    " '1.mirai.syn.csv':7,\n",
    " '1.mirai.udp.csv':8,\n",
    " '1.mirai.udpplain.csv':9}\n",
    "att.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e30e4447e465e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:42.538407Z",
     "start_time": "2025-07-18T05:18:26.341283Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m req:\n\u001B[1;32m----> 2\u001B[0m     data\u001B[38;5;241m=\u001B[39m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfdata/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m=\u001B[39matt_label[i]\n\u001B[0;32m      4\u001B[0m     att\u001B[38;5;241m=\u001B[39mpd\u001B[38;5;241m.\u001B[39mconcat([att,data],ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    900\u001B[0m     dialect,\n\u001B[0;32m    901\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    909\u001B[0m )\n\u001B[0;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    582\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 583\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1697\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1698\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1699\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1700\u001B[0m     (\n\u001B[0;32m   1701\u001B[0m         index,\n\u001B[0;32m   1702\u001B[0m         columns,\n\u001B[0;32m   1703\u001B[0m         col_dict,\n\u001B[1;32m-> 1704\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1705\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1708\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39mread_low_memory(nrows)\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:812\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:889\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1034\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1088\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1163\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1335\u001B[0m, in \u001B[0;36mis_extension_array_dtype\u001B[1;34m(arr_or_dtype)\u001B[0m\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001B[39;00m\n\u001B[0;32m   1327\u001B[0m     \u001B[38;5;66;03m#  here too.\u001B[39;00m\n\u001B[0;32m   1328\u001B[0m     \u001B[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001B[39;00m\n\u001B[0;32m   1329\u001B[0m     \u001B[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001B[39;00m\n\u001B[0;32m   1330\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, ExtensionDtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[0;32m   1331\u001B[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001B[0;32m   1332\u001B[0m     )\n\u001B[1;32m-> 1335\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_extension_array_dtype\u001B[39m(arr_or_dtype) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m   1336\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1337\u001B[0m \u001B[38;5;124;03m    Check if an object is a pandas extension array type.\u001B[39;00m\n\u001B[0;32m   1338\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1378\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[0;32m   1379\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   1380\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(arr_or_dtype, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, arr_or_dtype)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in req:\n",
    "    data=pd.read_csv(f'fdata/{i}')\n",
    "    data['label']=att_label[i]\n",
    "    att=pd.concat([att,data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce02235b4629525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:47.188731Z",
     "start_time": "2025-07-18T05:18:47.182432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(att.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123e62d0d1d8b6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:49.120188Z",
     "start_time": "2025-07-18T05:18:49.110968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496681, 116)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829eee622ee15842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:18:50.440896Z",
     "start_time": "2025-07-18T05:18:50.395578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MI_dir_L5_weight</th>\n",
       "      <th>MI_dir_L5_mean</th>\n",
       "      <th>MI_dir_L5_variance</th>\n",
       "      <th>MI_dir_L3_weight</th>\n",
       "      <th>MI_dir_L3_mean</th>\n",
       "      <th>MI_dir_L3_variance</th>\n",
       "      <th>MI_dir_L1_weight</th>\n",
       "      <th>MI_dir_L1_mean</th>\n",
       "      <th>MI_dir_L1_variance</th>\n",
       "      <th>MI_dir_L0.1_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>HpHp_L0.1_covariance</th>\n",
       "      <th>HpHp_L0.1_pcc</th>\n",
       "      <th>HpHp_L0.01_weight</th>\n",
       "      <th>HpHp_L0.01_mean</th>\n",
       "      <th>HpHp_L0.01_std</th>\n",
       "      <th>HpHp_L0.01_magnitude</th>\n",
       "      <th>HpHp_L0.01_radius</th>\n",
       "      <th>HpHp_L0.01_covariance</th>\n",
       "      <th>HpHp_L0.01_pcc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496679</th>\n",
       "      <td>81.231969</td>\n",
       "      <td>60.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>99.485449</td>\n",
       "      <td>60.000048</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>234.462476</td>\n",
       "      <td>60.003231</td>\n",
       "      <td>0.055332</td>\n",
       "      <td>2278.702317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496680</th>\n",
       "      <td>82.231365</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>100.485005</td>\n",
       "      <td>60.000047</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>235.462127</td>\n",
       "      <td>60.003217</td>\n",
       "      <td>0.055097</td>\n",
       "      <td>2279.701978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance   \n",
       "496679         81.231969       60.000001            0.000003  \\\n",
       "496680         82.231365       60.000000            0.000003   \n",
       "\n",
       "        MI_dir_L3_weight  MI_dir_L3_mean  MI_dir_L3_variance   \n",
       "496679         99.485449       60.000048            0.000288  \\\n",
       "496680        100.485005       60.000047            0.000286   \n",
       "\n",
       "        MI_dir_L1_weight  MI_dir_L1_mean  MI_dir_L1_variance   \n",
       "496679        234.462476       60.003231            0.055332  \\\n",
       "496680        235.462127       60.003217            0.055097   \n",
       "\n",
       "        MI_dir_L0.1_weight  ...  HpHp_L0.1_covariance  HpHp_L0.1_pcc   \n",
       "496679         2278.702317  ...                   0.0            0.0  \\\n",
       "496680         2279.701978  ...                   0.0            0.0   \n",
       "\n",
       "        HpHp_L0.01_weight  HpHp_L0.01_mean  HpHp_L0.01_std   \n",
       "496679                1.0             60.0             0.0  \\\n",
       "496680                1.0             60.0             0.0   \n",
       "\n",
       "        HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance   \n",
       "496679                  60.0                0.0                    0.0  \\\n",
       "496680                  60.0                0.0                    0.0   \n",
       "\n",
       "        HpHp_L0.01_pcc  label  \n",
       "496679             0.0    6.0  \n",
       "496680             0.0    6.0  \n",
       "\n",
       "[2 rows x 116 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26810472e2ddbe0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:20.980273Z",
     "start_time": "2025-05-11T13:02:20.974593Z"
    }
   },
   "outputs": [],
   "source": [
    "beg1['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bdb46b1c8e3c7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:26.050333Z",
     "start_time": "2025-05-11T13:02:21.023693Z"
    }
   },
   "outputs": [],
   "source": [
    "att=att.sample(frac=1).reset_index(drop=False)\n",
    "beg1=beg1.sample(frac=1).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a42a8943fff365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:26.916749Z",
     "start_time": "2025-05-11T13:02:26.222042Z"
    }
   },
   "outputs": [],
   "source": [
    "att=pd.concat([att,beg1],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ad6f0762eae091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:27.036720Z",
     "start_time": "2025-05-11T13:02:27.028265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(988449, 117)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb7135ec5d315603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:27.301558Z",
     "start_time": "2025-05-11T13:02:27.288389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'MI_dir_L5_weight', 'MI_dir_L5_mean', 'MI_dir_L5_variance',\n",
       "       'MI_dir_L3_weight', 'MI_dir_L3_mean', 'MI_dir_L3_variance',\n",
       "       'MI_dir_L1_weight', 'MI_dir_L1_mean', 'MI_dir_L1_variance',\n",
       "       ...\n",
       "       'HpHp_L0.1_covariance', 'HpHp_L0.1_pcc', 'HpHp_L0.01_weight',\n",
       "       'HpHp_L0.01_mean', 'HpHp_L0.01_std', 'HpHp_L0.01_magnitude',\n",
       "       'HpHp_L0.01_radius', 'HpHp_L0.01_covariance', 'HpHp_L0.01_pcc',\n",
       "       'label'],\n",
       "      dtype='object', length=117)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55178069938cc96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:35.803852Z",
     "start_time": "2025-05-11T13:02:27.842939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>MI_dir_L5_weight</th>\n",
       "      <th>MI_dir_L5_mean</th>\n",
       "      <th>MI_dir_L5_variance</th>\n",
       "      <th>MI_dir_L3_weight</th>\n",
       "      <th>MI_dir_L3_mean</th>\n",
       "      <th>MI_dir_L3_variance</th>\n",
       "      <th>MI_dir_L1_weight</th>\n",
       "      <th>MI_dir_L1_mean</th>\n",
       "      <th>MI_dir_L1_variance</th>\n",
       "      <th>...</th>\n",
       "      <th>HpHp_L0.1_covariance</th>\n",
       "      <th>HpHp_L0.1_pcc</th>\n",
       "      <th>HpHp_L0.01_weight</th>\n",
       "      <th>HpHp_L0.01_mean</th>\n",
       "      <th>HpHp_L0.01_std</th>\n",
       "      <th>HpHp_L0.01_magnitude</th>\n",
       "      <th>HpHp_L0.01_radius</th>\n",
       "      <th>HpHp_L0.01_covariance</th>\n",
       "      <th>HpHp_L0.01_pcc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>447159.693447</td>\n",
       "      <td>96.181595</td>\n",
       "      <td>196.218407</td>\n",
       "      <td>2.024997e+04</td>\n",
       "      <td>152.725293</td>\n",
       "      <td>197.006474</td>\n",
       "      <td>22616.385773</td>\n",
       "      <td>441.368719</td>\n",
       "      <td>198.289577</td>\n",
       "      <td>24143.647846</td>\n",
       "      <td>...</td>\n",
       "      <td>1.415425</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>762.810607</td>\n",
       "      <td>198.991450</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>201.650507</td>\n",
       "      <td>15.068967</td>\n",
       "      <td>3.622195</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>5.535973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>281432.614750</td>\n",
       "      <td>64.033486</td>\n",
       "      <td>164.307919</td>\n",
       "      <td>2.530590e+04</td>\n",
       "      <td>101.368937</td>\n",
       "      <td>157.495170</td>\n",
       "      <td>26893.439056</td>\n",
       "      <td>293.300065</td>\n",
       "      <td>153.448418</td>\n",
       "      <td>28021.805964</td>\n",
       "      <td>...</td>\n",
       "      <td>233.183840</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>3643.158012</td>\n",
       "      <td>218.741905</td>\n",
       "      <td>1.358988</td>\n",
       "      <td>217.752214</td>\n",
       "      <td>2019.218867</td>\n",
       "      <td>581.635407</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>2.602539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4338.246234</td>\n",
       "      <td>-0.541457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1017.176099</td>\n",
       "      <td>-0.541626</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>197564.000000</td>\n",
       "      <td>5.006255</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>3.456080e-11</td>\n",
       "      <td>5.135289</td>\n",
       "      <td>60.000034</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>5.667448</td>\n",
       "      <td>60.004848</td>\n",
       "      <td>0.038492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>444676.000000</td>\n",
       "      <td>112.609465</td>\n",
       "      <td>74.057992</td>\n",
       "      <td>4.215028e+01</td>\n",
       "      <td>189.410481</td>\n",
       "      <td>74.065092</td>\n",
       "      <td>44.913523</td>\n",
       "      <td>603.599856</td>\n",
       "      <td>74.118210</td>\n",
       "      <td>45.715982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>691788.000000</td>\n",
       "      <td>148.165814</td>\n",
       "      <td>337.718428</td>\n",
       "      <td>4.972245e+04</td>\n",
       "      <td>232.575255</td>\n",
       "      <td>351.372339</td>\n",
       "      <td>54223.666434</td>\n",
       "      <td>660.637654</td>\n",
       "      <td>367.871530</td>\n",
       "      <td>56234.778246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>938900.000000</td>\n",
       "      <td>341.681276</td>\n",
       "      <td>886.166851</td>\n",
       "      <td>1.736891e+05</td>\n",
       "      <td>470.490049</td>\n",
       "      <td>846.320306</td>\n",
       "      <td>172810.555548</td>\n",
       "      <td>979.356399</td>\n",
       "      <td>709.725273</td>\n",
       "      <td>173077.043447</td>\n",
       "      <td>...</td>\n",
       "      <td>57695.838248</td>\n",
       "      <td>0.933804</td>\n",
       "      <td>23444.559933</td>\n",
       "      <td>909.000000</td>\n",
       "      <td>203.672181</td>\n",
       "      <td>1278.265351</td>\n",
       "      <td>418453.161480</td>\n",
       "      <td>135794.306917</td>\n",
       "      <td>1.528392</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               index  MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance   \n",
       "count  988449.000000     988449.000000   988449.000000        9.884490e+05  \\\n",
       "mean   447159.693447         96.181595      196.218407        2.024997e+04   \n",
       "std    281432.614750         64.033486      164.307919        2.530590e+04   \n",
       "min         0.000000          1.000000       60.000000        0.000000e+00   \n",
       "25%    197564.000000          5.006255       60.000000        3.456080e-11   \n",
       "50%    444676.000000        112.609465       74.057992        4.215028e+01   \n",
       "75%    691788.000000        148.165814      337.718428        4.972245e+04   \n",
       "max    938900.000000        341.681276      886.166851        1.736891e+05   \n",
       "\n",
       "       MI_dir_L3_weight  MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight   \n",
       "count     988449.000000   988449.000000       988449.000000     988449.000000  \\\n",
       "mean         152.725293      197.006474        22616.385773        441.368719   \n",
       "std          101.368937      157.495170        26893.439056        293.300065   \n",
       "min            1.000000       60.000000            0.000000          1.000000   \n",
       "25%            5.135289       60.000034            0.000002          5.667448   \n",
       "50%          189.410481       74.065092           44.913523        603.599856   \n",
       "75%          232.575255      351.372339        54223.666434        660.637654   \n",
       "max          470.490049      846.320306       172810.555548        979.356399   \n",
       "\n",
       "       MI_dir_L1_mean  MI_dir_L1_variance  ...  HpHp_L0.1_covariance   \n",
       "count   988449.000000       988449.000000  ...         988449.000000  \\\n",
       "mean       198.289577        24143.647846  ...              1.415425   \n",
       "std        153.448418        28021.805964  ...            233.183840   \n",
       "min         60.000000            0.000000  ...          -4338.246234   \n",
       "25%         60.004848            0.038492  ...              0.000000   \n",
       "50%         74.118210           45.715982  ...              0.000000   \n",
       "75%        367.871530        56234.778246  ...              0.000000   \n",
       "max        709.725273       173077.043447  ...          57695.838248   \n",
       "\n",
       "       HpHp_L0.1_pcc  HpHp_L0.01_weight  HpHp_L0.01_mean  HpHp_L0.01_std   \n",
       "count  988449.000000      988449.000000    988449.000000   988449.000000  \\\n",
       "mean        0.000047         762.810607       198.991450        0.017180   \n",
       "std         0.005881        3643.158012       218.741905        1.358988   \n",
       "min        -0.541457           1.000000        60.000000        0.000000   \n",
       "25%         0.000000           1.000000        60.000000        0.000000   \n",
       "50%         0.000000           1.000000        60.000000        0.000000   \n",
       "75%         0.000000           1.000000       554.000000        0.000000   \n",
       "max         0.933804       23444.559933       909.000000      203.672181   \n",
       "\n",
       "       HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance   \n",
       "count         988449.000000      988449.000000          988449.000000  \\\n",
       "mean             201.650507          15.068967               3.622195   \n",
       "std              217.752214        2019.218867             581.635407   \n",
       "min               60.000000           0.000000           -1017.176099   \n",
       "25%               60.000000           0.000000               0.000000   \n",
       "50%               60.000000           0.000000               0.000000   \n",
       "75%              554.000000           0.000000               0.000000   \n",
       "max             1278.265351      418453.161480          135794.306917   \n",
       "\n",
       "       HpHp_L0.01_pcc          label  \n",
       "count   988449.000000  988449.000000  \n",
       "mean         0.000139       5.535973  \n",
       "std          0.010025       2.602539  \n",
       "min         -0.541626       0.000000  \n",
       "25%          0.000000       4.000000  \n",
       "50%          0.000000       6.000000  \n",
       "75%          0.000000       8.000000  \n",
       "max          1.528392       9.000000  \n",
       "\n",
       "[8 rows x 117 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fac8d69fddb1576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:44.462345Z",
     "start_time": "2025-05-11T13:02:35.825290Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = att.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = att.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdabed8e1a3342c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:49.403459Z",
     "start_time": "2025-05-11T13:02:44.613081Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Scale numerical features\n",
    "att[numerical_cols] = scaler.fit_transform(att[numerical_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5252cbb9df4a9ece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T13:02:58.409853Z",
     "start_time": "2025-05-11T13:02:49.524789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>MI_dir_L5_weight</th>\n",
       "      <th>MI_dir_L5_mean</th>\n",
       "      <th>MI_dir_L5_variance</th>\n",
       "      <th>MI_dir_L3_weight</th>\n",
       "      <th>MI_dir_L3_mean</th>\n",
       "      <th>MI_dir_L3_variance</th>\n",
       "      <th>MI_dir_L1_weight</th>\n",
       "      <th>MI_dir_L1_mean</th>\n",
       "      <th>MI_dir_L1_variance</th>\n",
       "      <th>...</th>\n",
       "      <th>HpHp_L0.1_covariance</th>\n",
       "      <th>HpHp_L0.1_pcc</th>\n",
       "      <th>HpHp_L0.01_weight</th>\n",
       "      <th>HpHp_L0.01_mean</th>\n",
       "      <th>HpHp_L0.01_std</th>\n",
       "      <th>HpHp_L0.01_magnitude</th>\n",
       "      <th>HpHp_L0.01_radius</th>\n",
       "      <th>HpHp_L0.01_covariance</th>\n",
       "      <th>HpHp_L0.01_pcc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>9.884490e+05</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "      <td>988449.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.476259</td>\n",
       "      <td>0.279386</td>\n",
       "      <td>1.648800e-01</td>\n",
       "      <td>1.165875e-01</td>\n",
       "      <td>0.323170</td>\n",
       "      <td>1.742375e-01</td>\n",
       "      <td>1.308739e-01</td>\n",
       "      <td>0.450111</td>\n",
       "      <td>0.212843</td>\n",
       "      <td>1.394965e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069956</td>\n",
       "      <td>0.367057</td>\n",
       "      <td>0.032496</td>\n",
       "      <td>1.637120e-01</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>1.162723e-01</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>0.261720</td>\n",
       "      <td>0.615108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.299747</td>\n",
       "      <td>0.187957</td>\n",
       "      <td>1.988798e-01</td>\n",
       "      <td>1.456965e-01</td>\n",
       "      <td>0.215913</td>\n",
       "      <td>2.002939e-01</td>\n",
       "      <td>1.556238e-01</td>\n",
       "      <td>0.299789</td>\n",
       "      <td>0.236174</td>\n",
       "      <td>1.619037e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>0.155401</td>\n",
       "      <td>2.576465e-01</td>\n",
       "      <td>0.006672</td>\n",
       "      <td>1.787396e-01</td>\n",
       "      <td>0.004825</td>\n",
       "      <td>0.004251</td>\n",
       "      <td>0.004843</td>\n",
       "      <td>0.289171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.210421</td>\n",
       "      <td>0.011760</td>\n",
       "      <td>1.419468e-10</td>\n",
       "      <td>1.989809e-16</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>4.325900e-08</td>\n",
       "      <td>1.089964e-11</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>2.223999e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069933</td>\n",
       "      <td>0.367025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.081668e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.261653</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.473614</td>\n",
       "      <td>0.327607</td>\n",
       "      <td>1.701592e-02</td>\n",
       "      <td>2.426767e-04</td>\n",
       "      <td>0.401309</td>\n",
       "      <td>1.788723e-02</td>\n",
       "      <td>2.599003e-04</td>\n",
       "      <td>0.615931</td>\n",
       "      <td>0.021730</td>\n",
       "      <td>2.641366e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069933</td>\n",
       "      <td>0.367025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775558e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.081668e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.261653</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.736807</td>\n",
       "      <td>0.431975</td>\n",
       "      <td>3.361530e-01</td>\n",
       "      <td>2.862728e-01</td>\n",
       "      <td>0.493248</td>\n",
       "      <td>3.705517e-01</td>\n",
       "      <td>3.137752e-01</td>\n",
       "      <td>0.674230</td>\n",
       "      <td>0.473849</td>\n",
       "      <td>3.249118e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069933</td>\n",
       "      <td>0.367025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.818610e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.054946e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.261653</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               index  MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance   \n",
       "count  988449.000000     988449.000000    9.884490e+05        9.884490e+05  \\\n",
       "mean        0.476259          0.279386    1.648800e-01        1.165875e-01   \n",
       "std         0.299747          0.187957    1.988798e-01        1.456965e-01   \n",
       "min         0.000000          0.000000    0.000000e+00        0.000000e+00   \n",
       "25%         0.210421          0.011760    1.419468e-10        1.989809e-16   \n",
       "50%         0.473614          0.327607    1.701592e-02        2.426767e-04   \n",
       "75%         0.736807          0.431975    3.361530e-01        2.862728e-01   \n",
       "max         1.000000          1.000000    1.000000e+00        1.000000e+00   \n",
       "\n",
       "       MI_dir_L3_weight  MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight   \n",
       "count     988449.000000    9.884490e+05        9.884490e+05     988449.000000  \\\n",
       "mean           0.323170    1.742375e-01        1.308739e-01          0.450111   \n",
       "std            0.215913    2.002939e-01        1.556238e-01          0.299789   \n",
       "min            0.000000    0.000000e+00        0.000000e+00          0.000000   \n",
       "25%            0.008808    4.325900e-08        1.089964e-11          0.004771   \n",
       "50%            0.401309    1.788723e-02        2.599003e-04          0.615931   \n",
       "75%            0.493248    3.705517e-01        3.137752e-01          0.674230   \n",
       "max            1.000000    1.000000e+00        1.000000e+00          1.000000   \n",
       "\n",
       "       MI_dir_L1_mean  MI_dir_L1_variance  ...  HpHp_L0.1_covariance   \n",
       "count   988449.000000        9.884490e+05  ...         988449.000000  \\\n",
       "mean         0.212843        1.394965e-01  ...              0.069956   \n",
       "std          0.236174        1.619037e-01  ...              0.003759   \n",
       "min          0.000000        0.000000e+00  ...              0.000000   \n",
       "25%          0.000007        2.223999e-07  ...              0.069933   \n",
       "50%          0.021730        2.641366e-04  ...              0.069933   \n",
       "75%          0.473849        3.249118e-01  ...              0.069933   \n",
       "max          1.000000        1.000000e+00  ...              1.000000   \n",
       "\n",
       "       HpHp_L0.1_pcc  HpHp_L0.01_weight  HpHp_L0.01_mean  HpHp_L0.01_std   \n",
       "count  988449.000000      988449.000000     9.884490e+05   988449.000000  \\\n",
       "mean        0.367057           0.032496     1.637120e-01        0.000084   \n",
       "std         0.003987           0.155401     2.576465e-01        0.006672   \n",
       "min         0.000000           0.000000     0.000000e+00        0.000000   \n",
       "25%         0.367025           0.000000     2.775558e-17        0.000000   \n",
       "50%         0.367025           0.000000     2.775558e-17        0.000000   \n",
       "75%         0.367025           0.000000     5.818610e-01        0.000000   \n",
       "max         1.000000           1.000000     1.000000e+00        1.000000   \n",
       "\n",
       "       HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance   \n",
       "count          9.884490e+05      988449.000000          988449.000000  \\\n",
       "mean           1.162723e-01           0.000036               0.007461   \n",
       "std            1.787396e-01           0.004825               0.004251   \n",
       "min            0.000000e+00           0.000000               0.000000   \n",
       "25%            2.081668e-17           0.000000               0.007435   \n",
       "50%            2.081668e-17           0.000000               0.007435   \n",
       "75%            4.054946e-01           0.000000               0.007435   \n",
       "max            1.000000e+00           1.000000               1.000000   \n",
       "\n",
       "       HpHp_L0.01_pcc          label  \n",
       "count   988449.000000  988449.000000  \n",
       "mean         0.261720       0.615108  \n",
       "std          0.004843       0.289171  \n",
       "min          0.000000       0.000000  \n",
       "25%          0.261653       0.444444  \n",
       "50%          0.261653       0.666667  \n",
       "75%          0.261653       0.888889  \n",
       "max          1.000000       1.000000  \n",
       "\n",
       "[8 rows x 117 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48e6e17036fff775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:23:43.605200Z",
     "start_time": "2025-07-18T05:19:15.951493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.4713 | Train Acc: 0.7737 | Val Loss: 0.2847 | Val Acc: 0.8429\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2395 | Train Acc: 0.8713 | Val Loss: 0.2072 | Val Acc: 0.8788\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1957 | Train Acc: 0.8825 | Val Loss: 0.1853 | Val Acc: 0.8686\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1762 | Train Acc: 0.8918 | Val Loss: 0.1709 | Val Acc: 0.8982\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1656 | Train Acc: 0.8959 | Val Loss: 0.1618 | Val Acc: 0.8997\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1582 | Train Acc: 0.8984 | Val Loss: 0.1543 | Val Acc: 0.9025\n",
      "Epoch 7/10\n",
      "Train Loss: 0.1542 | Train Acc: 0.8995 | Val Loss: 0.1510 | Val Acc: 0.9040\n",
      "Epoch 8/10\n",
      "Train Loss: 0.1513 | Train Acc: 0.9000 | Val Loss: 0.1510 | Val Acc: 0.8892\n",
      "Epoch 9/10\n",
      "Train Loss: 0.1685 | Train Acc: 0.8982 | Val Loss: 0.1617 | Val Acc: 0.9033\n",
      "Epoch 10/10\n",
      "Train Loss: 0.1539 | Train Acc: 0.9013 | Val Loss: 0.1516 | Val Acc: 0.9035\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load and preprocess data\n",
    "beg1 = pd.read_csv('fdata/1.benign.csv')\n",
    "lis = os.listdir('fdata')\n",
    "req = [lis[i] for i in range(1, 11) if lis[i] != '1.gafgyt.scan.csv']\n",
    "\n",
    "att_label = {\n",
    "    '1.gafgyt.combo.csv': 1,\n",
    "    '1.gafgyt.junk.csv': 2,\n",
    "    '1.gafgyt.tcp.csv': 3,\n",
    "    '1.gafgyt.udp.csv': 4,\n",
    "    '1.mirai.ack.csv': 5,\n",
    "    '1.mirai.scan.csv': 6,\n",
    "    '1.mirai.syn.csv': 7,\n",
    "    '1.mirai.udp.csv': 8,\n",
    "    '1.mirai.udpplain.csv': 9\n",
    "}\n",
    "\n",
    "att = pd.DataFrame(columns=beg1.columns)\n",
    "for i in req:\n",
    "    data = pd.read_csv(f'fdata/{i}')\n",
    "    data['label'] = att_label[i]\n",
    "    att = pd.concat([att, data], ignore_index=True)\n",
    "\n",
    "beg1['label'] = 0\n",
    "att = pd.concat([att, beg1], ignore_index=True)\n",
    "\n",
    "# Shuffle data\n",
    "att = att.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = att.drop(columns=['label'])\n",
    "y = att['label']\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Train-test split\n",
    "SEED = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(att['label'].unique())\n",
    "model = NeuralNetwork(input_size, num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for features, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc, val_preds, val_labels = evaluate(model, test_loader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_traffic_model.pt\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d4e19a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:33:50.321750Z",
     "start_time": "2025-07-18T05:33:23.632112Z"
    }
   },
   "source": [
    "# Data Loading and Preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load and preprocess data\n",
    "beg1 = pd.read_csv('fdata/1.benign.csv')\n",
    "lis = os.listdir('fdata')\n",
    "req = [lis[i] for i in range(1, 11) if lis[i] != '1.gafgyt.scan.csv']\n",
    "\n",
    "att_label = {\n",
    "    '1.gafgyt.combo.csv': 1,\n",
    "    '1.gafgyt.junk.csv': 2,\n",
    "    '1.gafgyt.tcp.csv': 3,\n",
    "    '1.gafgyt.udp.csv': 4,\n",
    "    '1.mirai.ack.csv': 5,\n",
    "    '1.mirai.scan.csv': 6,\n",
    "    '1.mirai.syn.csv': 7,\n",
    "    '1.mirai.udp.csv': 8,\n",
    "    '1.mirai.udpplain.csv': 9\n",
    "}\n",
    "\n",
    "att = pd.DataFrame(columns=beg1.columns)\n",
    "for i in req:\n",
    "    data = pd.read_csv(f'fdata/{i}')\n",
    "    data['label'] = att_label[i]\n",
    "    att = pd.concat([att, data], ignore_index=True)\n",
    "\n",
    "beg1['label'] = 0\n",
    "att = pd.concat([att, beg1], ignore_index=True)\n",
    "\n",
    "# Shuffle data\n",
    "att = att.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = att.drop(columns=['label'])\n",
    "y = att['label']\n",
    "\n",
    "# Scale numerical features\n",
    "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Train-test split\n",
    "SEED = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(att['label'].unique())}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (988449, 115)\n",
      "Number of classes: 10\n",
      "Training samples: 790759\n",
      "Test samples: 197690\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "8f3a9151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:34:13.974001Z",
     "start_time": "2025-07-18T05:34:13.907931Z"
    }
   },
   "source": [
    "# Neural Network Models - Different Architectures\n",
    "\n",
    "# Original NeuralNetwork\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64) \n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# NeuralNetwork1 - Deeper network with more layers\n",
    "class NeuralNetwork1(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# NeuralNetwork2 - Wider network with batch normalization\n",
    "class NeuralNetwork2(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# NeuralNetwork3 - Residual connections\n",
    "class NeuralNetwork3(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Linear(input_size, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        \n",
    "        # Add residual connection\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "# NeuralNetwork4 - LeakyReLU and different architecture\n",
    "class NeuralNetwork4(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNetwork4, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 50)\n",
    "        self.fc4 = nn.Linear(50, num_classes)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(att['label'].unique())\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Available models: NeuralNetwork, NeuralNetwork1, NeuralNetwork2, NeuralNetwork3, NeuralNetwork4\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 115\n",
      "Number of classes: 10\n",
      "Available models: NeuralNetwork, NeuralNetwork1, NeuralNetwork2, NeuralNetwork3, NeuralNetwork4\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "df8c82a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:34:32.227146Z",
     "start_time": "2025-07-18T05:34:32.191451Z"
    }
   },
   "source": [
    "# Training Loop - Test Different Models\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for features, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "def train_model(model_class, model_name, epochs=10, lr=1e-3, weight_decay=1e-4):\n",
    "    \"\"\"Train a specific model and return results\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = model_class(input_size, num_classes)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    results = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc, val_preds, val_labels = evaluate(model, test_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        results.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), f\"best_{model_name.lower()}_model.pt\")\n",
    "            print(f\"New best model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training functions ready! Uncomment the model you want to test above.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions ready! Uncomment the model you want to test above.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:41:23.483630Z",
     "start_time": "2025-07-18T05:35:05.405525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage - uncomment the model you want to test:\n",
    "\n",
    "# Test Original NeuralNetwork\n",
    "# model, results = train_model(NeuralNetwork, \"NeuralNetwork\")\n",
    "\n",
    "# Test NeuralNetwork1 (deeper network)\n",
    "model1, results1 = train_model(NeuralNetwork1, \"NeuralNetwork1\")\n",
    "\n",
    "# Test NeuralNetwork2 (wider with batch norm)\n",
    "# model2, results2 = train_model(NeuralNetwork2, \"NeuralNetwork2\")\n",
    "\n",
    "# Test NeuralNetwork3 (with residual connections)\n",
    "# model3, results3 = train_model(NeuralNetwork3, \"NeuralNetwork3\")\n",
    "\n",
    "# Test NeuralNetwork4 (LeakyReLU)\n",
    "# model4, results4 = train_model(NeuralNetwork4, \"NeuralNetwork4\")"
   ],
   "id": "ba41854107d0d744",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training NeuralNetwork1\n",
      "==================================================\n",
      "Epoch 1/10\n",
      "Train Loss: 0.4640 | Train Acc: 0.7595 | Val Loss: 0.2457 | Val Acc: 0.8532\n",
      "New best model saved!\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2271 | Train Acc: 0.8613 | Val Loss: 0.1916 | Val Acc: 0.8762\n",
      "New best model saved!\n",
      "Epoch 3/10\n",
      "Train Loss: 0.2026 | Train Acc: 0.8704 | Val Loss: 0.1847 | Val Acc: 0.8767\n",
      "New best model saved!\n",
      "Epoch 4/10\n",
      "Train Loss: 0.2012 | Train Acc: 0.8713 | Val Loss: 0.1838 | Val Acc: 0.8780\n",
      "New best model saved!\n",
      "Epoch 5/10\n",
      "Train Loss: 0.2021 | Train Acc: 0.8712 | Val Loss: 0.1830 | Val Acc: 0.8767\n",
      "New best model saved!\n",
      "Epoch 6/10\n",
      "Train Loss: 0.2112 | Train Acc: 0.8710 | Val Loss: 0.1930 | Val Acc: 0.8833\n",
      "Epoch 7/10\n",
      "Train Loss: 0.2138 | Train Acc: 0.8746 | Val Loss: 0.1815 | Val Acc: 0.8869\n",
      "New best model saved!\n",
      "Epoch 8/10\n",
      "Train Loss: 0.2172 | Train Acc: 0.8756 | Val Loss: 0.1761 | Val Acc: 0.8967\n",
      "New best model saved!\n",
      "Epoch 9/10\n",
      "Train Loss: 0.2225 | Train Acc: 0.8742 | Val Loss: 0.1883 | Val Acc: 0.8949\n",
      "Epoch 10/10\n",
      "Train Loss: 0.2196 | Train Acc: 0.8668 | Val Loss: 0.1613 | Val Acc: 0.9009\n",
      "New best model saved!\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T05:56:34.931491Z",
     "start_time": "2025-07-18T05:56:25.483840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage - uncomment the model you want to test:\n",
    "\n",
    "# Test Original NeuralNetwork\n",
    "# model, results = train_model(NeuralNetwork, \"NeuralNetwork\")\n",
    "\n",
    "# Test NeuralNetwork1 (deeper network)\n",
    "#model1, results1 = train_model(NeuralNetwork1, \"NeuralNetwork1\")\n",
    "\n",
    "# Test NeuralNetwork2 (wider with batch norm)\n",
    "model2, results2 = train_model(NeuralNetwork2, \"NeuralNetwork2\")\n",
    "\n",
    "# Test NeuralNetwork3 (with residual connections)\n",
    "# model3, results3 = train_model(NeuralNetwork3, \"NeuralNetwork3\")\n",
    "\n",
    "# Test NeuralNetwork4 (LeakyReLU)\n",
    "# model4, results4 = train_model(NeuralNetwork4, \"NeuralNetwork4\")"
   ],
   "id": "1829a508737e0fb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training NeuralNetwork2\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 10\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Example usage - uncomment the model you want to test:\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Test Original NeuralNetwork\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      8\u001B[0m \n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Test NeuralNetwork2 (wider with batch norm)\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m model2, results2 \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNeuralNetwork2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNeuralNetwork2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Test NeuralNetwork3 (with residual connections)\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# model3, results3 = train_model(NeuralNetwork3, \"NeuralNetwork3\")\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Test NeuralNetwork4 (LeakyReLU)\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# model4, results4 = train_model(NeuralNetwork4, \"NeuralNetwork4\")\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[17], line 71\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model_class, model_name, epochs, lr, weight_decay)\u001B[0m\n\u001B[0;32m     68\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m---> 71\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m     val_loss, val_acc, val_preds, val_labels \u001B[38;5;241m=\u001B[39m evaluate(model, test_loader, criterion)\n\u001B[0;32m     73\u001B[0m     scheduler\u001B[38;5;241m.\u001B[39mstep(val_loss)\n",
      "Cell \u001B[1;32mIn[17], line 11\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, dataloader, optimizer, criterion)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m features, labels \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[0;32m     10\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 11\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     13\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[16], line 58\u001B[0m, in \u001B[0;36mNeuralNetwork2.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     56\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn1(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x)))\n\u001B[0;32m     57\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[1;32m---> 58\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     59\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[0;32m     60\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(x)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T06:00:59.948097Z",
     "start_time": "2025-07-18T05:56:43.524044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage - uncomment the model you want to test:\n",
    "\n",
    "# Test Original NeuralNetwork\n",
    "# model, results = train_model(NeuralNetwork, \"NeuralNetwork\")\n",
    "\n",
    "# Test NeuralNetwork1 (deeper network)\n",
    "#model1, results1 = train_model(NeuralNetwork1, \"NeuralNetwork1\")\n",
    "\n",
    "# Test NeuralNetwork2 (wider with batch norm)\n",
    "#model2, results2 = train_model(NeuralNetwork2, \"NeuralNetwork2\")\n",
    "\n",
    "# Test NeuralNetwork3 (with residual connections)\n",
    "#model3, results3 = train_model(NeuralNetwork3, \"NeuralNetwork3\")\n",
    "\n",
    "# Test NeuralNetwork4 (LeakyReLU)\n",
    "model4, results4 = train_model(NeuralNetwork4, \"NeuralNetwork4\")"
   ],
   "id": "48afce0039f8218c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training NeuralNetwork4\n",
      "==================================================\n",
      "Epoch 1/10\n",
      "Train Loss: 0.4515 | Train Acc: 0.7678 | Val Loss: 0.2407 | Val Acc: 0.8633\n",
      "New best model saved!\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2262 | Train Acc: 0.8642 | Val Loss: 0.1967 | Val Acc: 0.8763\n",
      "New best model saved!\n",
      "Epoch 3/10\n",
      "Train Loss: 0.1993 | Train Acc: 0.8734 | Val Loss: 0.1877 | Val Acc: 0.8798\n",
      "New best model saved!\n",
      "Epoch 4/10\n",
      "Train Loss: 0.1920 | Train Acc: 0.8763 | Val Loss: 0.1810 | Val Acc: 0.8850\n",
      "New best model saved!\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1943 | Train Acc: 0.8767 | Val Loss: 0.1803 | Val Acc: 0.8854\n",
      "New best model saved!\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1844 | Train Acc: 0.8812 | Val Loss: 0.1771 | Val Acc: 0.8869\n",
      "New best model saved!\n",
      "Epoch 7/10\n",
      "Train Loss: 0.1842 | Train Acc: 0.8830 | Val Loss: 0.1712 | Val Acc: 0.8918\n",
      "New best model saved!\n",
      "Epoch 8/10\n",
      "Train Loss: 0.1766 | Train Acc: 0.8872 | Val Loss: 0.1646 | Val Acc: 0.8960\n",
      "New best model saved!\n",
      "Epoch 9/10\n",
      "Train Loss: 0.1767 | Train Acc: 0.8885 | Val Loss: 0.1601 | Val Acc: 0.8977\n",
      "New best model saved!\n",
      "Epoch 10/10\n",
      "Train Loss: 0.1699 | Train Acc: 0.8925 | Val Loss: 0.1583 | Val Acc: 0.9007\n",
      "New best model saved!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048e74c7a603c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Client 1: type1 attack,benign\n",
    "Client 2: --\n",
    "Clentt n:\n",
    "\n",
    "data scaling -> train notification -> local training -> global model update -> aggregated model \n",
    "\n",
    "\n",
    "Server: 2 \n",
    "Flower(python) \n",
    "\n",
    "up and run -> train notification (Ip addr Loop back) -> FedAvg (a,b,c) -> testing\n",
    "-> Save global \n",
    "\n",
    "\n",
    "** Diffrential privacy :Adding noise to to the params.\n",
    "** RealTime: focus\n",
    "** LightModel: 900mb \n",
    "** Heterogineous device:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "23a8e91566cbeaf4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
