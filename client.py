import osimport torchimport torch.nn as nnimport loggingimport numpy as npimport pandas as pdfrom typing import Dict, List, Tuplefrom collections import OrderedDictimport argparseimport globimport flwr as flfrom flwr.common import NDArrays, FitIns, EvaluateIns, FitRes, EvaluateRes, Parameters, ndarrays_to_parameters, \    parameters_to_ndarraysfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitfrom torch.utils.data import TensorDataset, DataLoader# Configure logginglogging.basicConfig(    level=logging.INFO,    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',    handlers=[        logging.FileHandler("client_logs.log"),        logging.StreamHandler()    ])# Attack labels mappingATT_LABEL = {    '1.gafgyt.combo.csv': 1,    '1.gafgyt.junk.csv': 2,    '1.gafgyt.tcp.csv': 3,    '1.gafgyt.udp.csv': 4,    '1.mirai.ack.csv': 5,    '1.mirai.scan.csv': 6,    '1.mirai.syn.csv': 7,    '1.mirai.udp.csv': 8,    '1.mirai.udpplain.csv': 9}# Model definition according to the specified architectureclass FlModel(nn.Module):    def __init__(self, input_size=115, num_classes=10):        super(FlModel, self).__init__()        self.model = nn.Sequential(            nn.Linear(input_size, 256),  # First hidden layer            nn.BatchNorm1d(256),  # Batch normalization            nn.ReLU(),  # Activation            nn.Dropout(0.3),  # Dropout for regularization            nn.Linear(256, 128),  # Second hidden layer            nn.BatchNorm1d(128),  # Batch normalization            nn.ReLU(),  # Activation            nn.Dropout(0.3),  # Dropout for regularization            nn.Linear(128, 64),  # Third hidden layer            nn.BatchNorm1d(64),  # Batch normalization            nn.ReLU(),  # Activation            nn.Dropout(0.3),  # Dropout for regularization            nn.Linear(64, num_classes)  # Output layer        )    def forward(self, x):        return self.model(x)class IoTClient(fl.client.NumPyClient):    def __init__(self, client_id: int, device: torch.device):        self.client_id = client_id        self.device = device        self.logger = logging.getLogger(f"Client-{client_id}")        # Initialize model        self.input_size = 115  # Default size for IoT data features        self.num_classes = 10  # 0-9 classes (including benign as 0)        self.model = FlModel(self.input_size, self.num_classes).to(device)        self.logger.info(f"Client {client_id} initialized with device: {device}")        # Setup dataset        self.trainloader, self.testloader = self.load_data()        # Define loss function and optimizer        self.criterion = torch.nn.CrossEntropyLoss()        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)    def load_data(self):        """        Load client-specific IoT dataset based on client ID.        Each client loads all files starting with their ID from the fdata/ directory.        """        try:            self.logger.info(f"Client {self.client_id} loading dataset...")            # Path to data directory            data_dir = "fdata/"            # Get all files for this client            file_pattern = f"{self.client_id}*.csv"            client_files = glob.glob(os.path.join(data_dir, file_pattern))            # Filter out specific files if needed (e.g., gafgytscan.csv)            # client_files = [f for f in client_files if "gafgytscan.csv" not in f]            if not client_files:                self.logger.error(f"No data files found for client {self.client_id}")                raise FileNotFoundError(f"No data files found for client {self.client_id}")            self.logger.info(f"Found {len(client_files)} files for client {self.client_id}: {client_files}")            # Process all CSV files and combine them            all_data = []            all_labels = []            for file_path in client_files:                file_name = os.path.basename(file_path)                # Determine label from filename                attack_type = None                for key in ATT_LABEL:                    if key in file_name:                        attack_type = ATT_LABEL[key]                        break                # If no attack type found, consider it benign (label 0)                if attack_type is None:                    attack_type = 0                # Read data                try:                    df = pd.read_csv(file_path)                    self.logger.info(f"Loaded file {file_path} with shape {df.shape}")                    # Remove any non-feature columns if they exist                    if 'label' in df.columns:                        df = df.drop('label', axis=1)                    # Store the input size for the model                    if len(all_data) == 0:                        self.input_size = df.shape[1]                        self.logger.info(f"Setting input size to {self.input_size}")                    # Add data and labels                    all_data.append(df.values)                    all_labels.extend([attack_type] * len(df))                except Exception as e:                    self.logger.error(f"Error processing file {file_path}: {e}")            if not all_data:                self.logger.error("No data could be loaded")                raise ValueError("No data could be loaded")            # Combine all data            X = np.vstack(all_data)            y = np.array(all_labels)            self.logger.info(f"Combined data shape: {X.shape}, labels shape: {y.shape}")            # Normalize features            scaler = StandardScaler()            X = scaler.fit_transform(X)            # Split into train and test sets            X_train, X_test, y_train, y_test = train_test_split(                X, y, test_size=0.2, random_state=42, stratify=y            )            # Convert to PyTorch tensors            X_train_tensor = torch.FloatTensor(X_train)            y_train_tensor = torch.LongTensor(y_train)            X_test_tensor = torch.FloatTensor(X_test)            y_test_tensor = torch.LongTensor(y_test)            # Create data loaders            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)            trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)            testloader = DataLoader(test_dataset, batch_size=32)            # Recreate model with correct input size            self.model = FlModel(self.input_size, self.num_classes).to(self.device)            self.logger.info(                f"Client {self.client_id} loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples"            )            self.logger.info(f"Data has {self.input_size} features and {self.num_classes} classes")            return trainloader, testloader        except Exception as e:            self.logger.error(f"Error loading data for client {self.client_id}: {e}")            raise    def get_parameters(self, config) -> List[np.ndarray]:        """Get model parameters as a list of NumPy arrays."""        self.logger.debug(f"Client {self.client_id} getting model parameters")        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]    def set_parameters(self, parameters: List[np.ndarray]) -> None:        """Set model parameters from a list of NumPy arrays."""        self.logger.debug(f"Client {self.client_id} setting model parameters")        params_dict = zip(self.model.state_dict().keys(), parameters)        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})        self.model.load_state_dict(state_dict, strict=True)    def fit(self, parameters: List[np.ndarray], config: Dict[str, str]) -> Tuple[List[np.ndarray], int, Dict]:        """Train the model on the local dataset."""        self.logger.info(f"Client {self.client_id} starting local training")        # Update local model with global parameters        self.set_parameters(parameters)        # Get training config from server        epochs = int(config.get("epochs", 1))        # Train the model        self.model.train()        for epoch in range(epochs):            running_loss = 0.0            correct = 0            total = 0            for batch_idx, (inputs, targets) in enumerate(self.trainloader):                inputs, targets = inputs.to(self.device), targets.to(self.device)                self.optimizer.zero_grad()                outputs = self.model(inputs)                loss = self.criterion(outputs, targets)                loss.backward()                self.optimizer.step()                # Compute metrics                running_loss += loss.item()                _, predicted = outputs.max(1)                total += targets.size(0)                correct += predicted.eq(targets).sum().item()            # Log epoch metrics            accuracy = correct / total            avg_loss = running_loss / len(self.trainloader)            self.logger.info(                f"Client {self.client_id} - Epoch {epoch + 1}/{epochs}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}"            )        # Return updated model parameters and metrics        final_accuracy = correct / total        return self.get_parameters(config={}), total, {"accuracy": float(final_accuracy), "loss": float(avg_loss)}    def evaluate(self, parameters: List[np.ndarray], config: Dict[str, str]) -> Tuple[float, int, Dict]:        """Evaluate the model on the local test dataset."""        self.logger.info(f"Client {self.client_id} evaluating model")        # Update local model with global parameters        self.set_parameters(parameters)        # Evaluate the model        self.model.eval()        loss = 0.0        correct = 0        total = 0        with torch.no_grad():            for inputs, targets in self.testloader:                inputs, targets = inputs.to(self.device), targets.to(self.device)                outputs = self.model(inputs)                batch_loss = self.criterion(outputs, targets).item()                loss += batch_loss                _, predicted = outputs.max(1)                total += targets.size(0)                correct += predicted.eq(targets).sum().item()        # Compute metrics        accuracy = correct / total        avg_loss = loss / len(self.testloader)        self.logger.info(f"Client {self.client_id} evaluation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")        return float(avg_loss), total, {"accuracy": float(accuracy)}def main():    # Parse command line arguments    parser = argparse.ArgumentParser(description="Flower IoT client")    parser.add_argument("--client-id", type=int, required=True, help="Client ID")    parser.add_argument("--server-address", type=str, default="127.0.0.1:8080", help="Server address")    args = parser.parse_args()    # Set device    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")    logger = logging.getLogger(f"Client-{args.client_id}")    logger.info(f"Using device: {device}")    # Create client    client = IoTClient(client_id=args.client_id, device=device)    # Start client    logger.info(f"Client {args.client_id} connecting to server at {args.server_address}")    fl.client.start_numpy_client(server_address=args.server_address, client=client)if __name__ == "__main__":    main()